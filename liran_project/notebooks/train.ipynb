{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for cauchy multiplication not found. Install by going to extensions/cauchy/ and running `python setup.py install`. This should speed up end-to-end training by 10-50%\n",
      "Falling back on slow Cauchy kernel. Install at least one of pykeops or the CUDA extension for efficiency.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "ProjectPath = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "sys.path.append(ProjectPath)  # Add the parent directory to the sys.path\n",
    "\n",
    "import liran_project.utils.dataset_loader as dataset_loader\n",
    "\n",
    "import liran_project.train as liran_train\n",
    "\n",
    "import liran_project.train as src_train\n",
    "import h5py"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:14:03.909990700Z",
     "start_time": "2023-12-25T20:13:52.625856600Z"
    }
   },
   "id": "2c94a53c3a1ee253"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[37mrambo5                       \u001B[m  Mon Dec 25 22:14:05 2023  \u001B[1m\u001B[30m525.116.04\u001B[m\r\n",
      "\u001B[36m[0]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 26°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m 9165\u001B[m / \u001B[33m11264\u001B[m MB | \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m502M\u001B[m) \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m6722M\u001B[m) \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m502M\u001B[m) \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m814M\u001B[m) \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m622M\u001B[m)\r\n",
      "\u001B[36m[1]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 24°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n",
      "\u001B[36m[2]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 26°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n",
      "\u001B[36m[3]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 26°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n",
      "\u001B[36m[4]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 24°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n",
      "\u001B[36m[5]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 25°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n",
      "\u001B[36m[6]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 26°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m 1975\u001B[m / \u001B[33m11264\u001B[m MB | \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m912M\u001B[m) \u001B[1m\u001B[30mliranc6\u001B[m(\u001B[33m1060M\u001B[m)\r\n",
      "\u001B[36m[7]\u001B[m \u001B[34mNVIDIA GeForce RTX 2080 Ti\u001B[m |\u001B[31m 26°C\u001B[m, \u001B[32m  0 %\u001B[m | \u001B[1m\u001B[33m    3\u001B[m / \u001B[33m11264\u001B[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "! gpustat\n",
    "\n",
    "subset_data_dir = \"/home/liranc6/ecg/ecg_forecasting/data/icentia11k-continuous-ecg_normal_sinus_subset/\" #patients 0-8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:14:05.123678800Z",
     "start_time": "2023-12-25T20:14:03.927936100Z"
    }
   },
   "id": "66e6e18c12a60ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../SSSD_main')\n",
    "\n",
    "from SSSD_main.src.utils.util import find_max_epoch, print_size, calc_diffusion_hyperparams #, training_loss\n",
    "from SSSD_main.src.utils.util import get_mask_mnr, get_mask_bm, get_mask_rm, get_mask_fm\n",
    "import SSSD_main.src.utils.util as util\n",
    "from SSSD_main.src.imputers.DiffWaveImputer import DiffWaveImputer\n",
    "from SSSD_main.src.imputers.SSSDSAImputer import SSSDSAImputer\n",
    "from SSSD_main.src.imputers.SSSDS4Imputer import SSSDS4Imputer\n",
    "\n",
    "# Import your custom dataset class here\n",
    "from liran_project.utils.dataset_loader import SingleLeadECGDatasetCrops as CustomDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:14:05.144593700Z",
     "start_time": "2023-12-25T20:14:05.126649800Z"
    }
   },
   "id": "a5259350125576ed"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets_sizes=[165, 99, 56, 33, 36, 85, 70, 164, 80]\n"
     ]
    }
   ],
   "source": [
    "# split the windows to fixed size context and label windows\n",
    "fs = 250\n",
    "context_window_size = 9*60*fs  # minutes * seconds * fs\n",
    "label_window_size = 1*60*fs  # minutes * seconds * fs\n",
    "window_size = context_window_size+label_window_size\n",
    "\n",
    "\n",
    "ten_minutes_window_file = '/home/liranc6/ecg/ecg_forecasting/data/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes_window.h5'\n",
    "\n",
    "# Instantiate the class\n",
    "dataset = dataset_loader.SingleLeadECGDatasetCrops(context_window_size, label_window_size, ten_minutes_window_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:14:05.207417300Z",
     "start_time": "2023-12-25T20:14:05.136606700Z"
    }
   },
   "id": "ca56c6c474d36303"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liranc6/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid SSSDS4 model found. Initializing from scratch.\n",
      "No valid SSSDSA model found. Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration files\n",
    "config_SSSDS4_path = os.path.join(ProjectPath, 'SSSD_main', 'src','config','config_SSSDS4.json') \n",
    "config_SSSDSA_path = os.path.join(ProjectPath, 'SSSD_main', 'src','config','config_SSSDSA.json') \n",
    "\n",
    "with open(config_SSSDS4_path) as f:\n",
    "    config_SSSDS4 = json.load(f)\n",
    "\n",
    "with open(config_SSSDSA_path) as f:\n",
    "    config_SSSDSA = json.load(f)\n",
    "\n",
    "# Parse necessary configurations for SSSDS4\n",
    "gen_config_SSSDS4 = config_SSSDS4['gen_config']\n",
    "train_config_SSSDS4 = config_SSSDS4['train_config']\n",
    "trainset_config_SSSDS4 = config_SSSDS4['trainset_config']\n",
    "diffusion_config_SSSDS4 = config_SSSDS4['diffusion_config']\n",
    "wavenet_config_SSSDS4 = config_SSSDS4['wavenet_config']\n",
    "\n",
    "# Parse necessary configurations for SSSDSA\n",
    "gen_config_SSSDSA = config_SSSDSA['gen_config']\n",
    "train_config_SSSDSA = config_SSSDSA['train_config']\n",
    "trainset_config_SSSDSA = config_SSSDSA['trainset_config']\n",
    "diffusion_config_SSSDSA = config_SSSDSA['diffusion_config']\n",
    "sashimi_config_SSSDSA = config_SSSDSA['sashimi_config']\n",
    "\n",
    "# Load your custom datasets\n",
    "train_dataset_SSSDS4 = dataset\n",
    "train_loader_SSSDS4 = DataLoader(train_dataset_SSSDS4, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "train_dataset_SSSDSA = dataset\n",
    "train_loader_SSSDSA = DataLoader(train_dataset_SSSDSA, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize your models and optimizers based on the chosen 'use_model'\n",
    "net_SSSDS4 = SSSDS4Imputer(**wavenet_config_SSSDS4).cuda()\n",
    "optimizer_SSSDS4 = torch.optim.Adam(net_SSSDS4.parameters(), lr=train_config_SSSDS4['learning_rate'])\n",
    "\n",
    "net_SSSDSA = SSSDSAImputer(**sashimi_config_SSSDSA).cuda()\n",
    "optimizer_SSSDSA = torch.optim.Adam(net_SSSDSA.parameters(), lr=train_config_SSSDSA['learning_rate'])\n",
    "\n",
    "# Load checkpoints if available for both models\n",
    "ckpt_path_SSSDS4 = os.path.join(train_config_SSSDS4[\"output_directory\"], \"T{}_beta0{}_betaT{}\".format(\n",
    "    diffusion_config_SSSDS4[\"T\"], diffusion_config_SSSDS4[\"beta_0\"], diffusion_config_SSSDS4[\"beta_T\"]))\n",
    "ckpt_path_SSSDSA = train_config_SSSDSA[\"output_directory\"]\n",
    "\n",
    "args = type('Arguments', (object,), {'ckpt_iter': 'max'})  # Mock argparse arguments\n",
    "args.ckpt_iter = 'max'\n",
    "\n",
    "model_path_SSSDS4 = os.path.join(ckpt_path_SSSDS4, '{}.pkl'.format(args.ckpt_iter))\n",
    "model_path_SSSDSA = os.path.join(ckpt_path_SSSDSA, '{}.pkl'.format(args.ckpt_iter))\n",
    "\n",
    "try:\n",
    "    checkpoint_SSSDS4 = torch.load(model_path_SSSDS4, map_location='cpu')\n",
    "    net_SSSDS4.load_state_dict(checkpoint_SSSDS4['model_state_dict'])\n",
    "    optimizer_SSSDS4.load_state_dict(checkpoint_SSSDS4['optimizer_state_dict'])\n",
    "    print('Successfully loaded SSSDS4 model at iteration {}'.format(args.ckpt_iter))\n",
    "except:\n",
    "    print('No valid SSSDS4 model found. Initializing from scratch.')\n",
    "try:\n",
    "    checkpoint_SSSDSA = torch.load(model_path_SSSDSA, map_location='cpu')\n",
    "    net_SSSDSA.load_state_dict(checkpoint_SSSDSA['model_state_dict'])\n",
    "    optimizer_SSSDSA.load_state_dict(checkpoint_SSSDSA['optimizer_state_dict'])\n",
    "    print('Successfully loaded SSSDSA model at iteration {}'.format(args.ckpt_iter))\n",
    "except:\n",
    "    print('No valid SSSDSA model found. Initializing from scratch.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:14:23.460577600Z",
     "start_time": "2023-12-25T20:14:05.208418100Z"
    }
   },
   "id": "ed2dc154f57f7b91"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train(output_directory,\n",
    "          ckpt_iter, \n",
    "          n_iters, \n",
    "          iters_per_ckpt,\n",
    "          iters_per_logging,\n",
    "          learning_rate,\n",
    "          only_generate_missing,\n",
    "          masking,\n",
    "          missing_k,\n",
    "          net,\n",
    "          diffusion_config,\n",
    "          diffusion_hyperparams,\n",
    "          trainset_config,\n",
    "          context_size,\n",
    "          label_size,\n",
    "          **kwargs):\n",
    "    \"\"\"\n",
    "    Train Diffusion Models\n",
    "\n",
    "    This function trains diffusion models using the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    output_directory (str):         Path to save model checkpoints.\n",
    "    ckpt_iter (int or 'max'):       The pretrained checkpoint to be loaded. \n",
    "                                    If 'max' is selected, it automatically selects the maximum iteration.\n",
    "    n_iters (int):                  Number of iterations to train.\n",
    "    iters_per_ckpt (int):           Number of iterations to save checkpoint. \n",
    "                                    Default is 10k, for models with residual_channel=64 this number can be larger.\n",
    "    iters_per_logging (int):        Number of iterations to save training log and compute validation loss. Default is 100.\n",
    "    learning_rate (float):          Learning rate.\n",
    "    use_model (int):                Model selection:\n",
    "                                    0: DiffWave.\n",
    "                                    1: SSSDSA.\n",
    "                                    2: SSSDS4.\n",
    "    only_generate_missing (int):    0: Apply diffusion to all samples.\n",
    "                                    1: Only apply diffusion to missing portions of the signal.\n",
    "    masking (str):                  Masking strategy:\n",
    "                                    'mnr': Missing not at random.\n",
    "                                    'bm': Blackout missing.\n",
    "                                    'rm': Random missing.\n",
    "    missing_k (int):                Number of missing time steps for each feature across the sample length.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate experiment (local) path\n",
    "    local_path = \"T{}_beta0{}_betaT{}\".format(diffusion_config[\"T\"],\n",
    "                                              diffusion_config[\"beta_0\"],\n",
    "                                              diffusion_config[\"beta_T\"])\n",
    "\n",
    "    # Get shared output_directory ready\n",
    "    output_directory = os.path.join(output_directory, local_path)\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        os.chmod(output_directory, 0o775)\n",
    "    print(\"output directory\", output_directory, flush=True)\n",
    "\n",
    "    # map diffusion hyperparameters to gpu\n",
    "    for key in diffusion_hyperparams:\n",
    "        if key != \"T\":\n",
    "            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "\n",
    "    # predefine model\n",
    "    net = net.cuda()\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # load checkpoint\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(output_directory)\n",
    "    if ckpt_iter >= 0:\n",
    "        try:\n",
    "            # load checkpoint file\n",
    "            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "            # feed model dict and optimizer state\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print('Successfully loaded model at iteration {}'.format(ckpt_iter))\n",
    "        except:\n",
    "            ckpt_iter = -1\n",
    "            print('No valid checkpoint model found, start training from initialization try.')\n",
    "    else:\n",
    "        ckpt_iter = -1\n",
    "        print('No valid checkpoint model found, start training from initialization.')\n",
    "\n",
    "    \n",
    "    def load_first_dataset(file_path):\n",
    "        \"\"\"Load data from the first dataset of an H5 file.\"\"\"\n",
    "        with h5py.File(file_path, 'r') as h5_file:\n",
    "            # Assuming the datasets are named as '/00000', '/00001', etc.\n",
    "            first_dataset = h5_file['/00000'][()]\n",
    "        return first_dataset\n",
    "    \n",
    "    # Specify the path to the H5 file\n",
    "    file_path = '/home/liranc6/ecg/ecg_forecasting/data/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes_window.h5'\n",
    "    # Load data from the first dataset\n",
    "    first_dataset = load_first_dataset(file_path)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor, make it a float tensor,\n",
    "    # and move it to the GPU if available\n",
    "    training_data = torch.from_numpy(first_dataset).float().cuda()\n",
    "    # print(f'{training_data.shape=}')\n",
    "    training_data = training_data[:1, :] #take only the first 20 patients and 100 time steps to save memory\n",
    "    training_data.unsqueeze_(1) #split the data to batches of size 1\n",
    "    training_data.unsqueeze_(1) #add a channel dimension\n",
    "    \n",
    "    training_data = training_data.permute(0, 1, 3, 2) # the code expects the data to be in the shape of (batch_size, sequence_length, channels)\n",
    "    \n",
    "    print(f'{training_data.shape=}')\n",
    "    \n",
    "    \n",
    "    # ### Custom data loading and reshaping ###\n",
    "    # training_data = np.load(trainset_config['train_data_path'])\n",
    "    # \n",
    "    # training_data = np.split(training_data, 160, 0) # Split the array into 160 equal sub-arrays along the first axis\n",
    "    #                                                 # If the array cannot be evenly divided into 160 sub-arrays, a ValueError will be raised.\n",
    "    # \n",
    "    # training_data = np.array(training_data) # Convert the list of NumPy arrays into a single multi-dim NumPy array\n",
    "    # training_data = torch.from_numpy(training_data).float().cuda() # Convert the NumPy array to a PyTorch tensor, make it a float tensor, and move it to the GPU if available\n",
    "    print('Data loaded')\n",
    "\n",
    "    # training\n",
    "    n_iter = ckpt_iter + 1\n",
    "    while n_iter < n_iters + 1:\n",
    "        for i, batch in enumerate(training_data):\n",
    "            \n",
    "            if n_iter % 20 == 0:\n",
    "                print(f'{n_iter=}')\n",
    "            if i % 10 == 0:\n",
    "                print(f'{i=}')\n",
    "\n",
    "            # TODO: what is the porpuse and use of the masking in here?\n",
    "            \"\"\"\n",
    "            copilot answer:\n",
    "            In this code, masking is used to selectively ignore or pay attention to certain elements of the data during the training process.\n",
    "            The mask is a tensor of the same shape as the input data, where each element of the mask corresponds to an element of the input data. \n",
    "\n",
    "            The type of mask applied depends on the `masking` variable, which can be 'rm', 'mnr', or 'bm'. Each of these values corresponds to a\n",
    "            different masking strategy, implemented by the `get_mask_rm`, `get_mask_mnr`, and `get_mask_bm` functions respectively.\n",
    "\n",
    "            Once the mask is created, it is permuted, repeated across the batch size, and converted to a float tensor on the GPU with `.float().cuda()`.\n",
    "            The `loss_mask` is the logical negation of `mask`, converted to a boolean tensor with `.bool()`. \n",
    "            This means that wherever `mask` is True, `loss_mask` is False, and vice versa.\n",
    "\n",
    "            The `mask` and `loss_mask` are then used in the `training_loss` function. While the exact usage depends on the implementation of\n",
    "            `training_loss`, typically, elements of the input data where `mask` is True are ignored or treated differently during the computation\n",
    "            of the loss. Conversely, elements where `loss_mask` is True are used normally. This allows the model to focus on certain parts of the\n",
    "            data while ignoring others, which can be useful in many machine learning tasks.\n",
    "            \"\"\"\n",
    "            transposed_mask = None\n",
    "            if masking == 'rm':\n",
    "                transposed_mask = get_mask_rm(batch[0], missing_k)\n",
    "            elif masking == 'mnr':\n",
    "                transposed_mask = get_mask_mnr(batch[0], missing_k)\n",
    "            elif masking == 'bm':\n",
    "                transposed_mask = get_mask_bm(batch[0], missing_k)\n",
    "            elif masking == 'fm':\n",
    "                transposed_mask = get_mask_fm(batch[0], context_size, label_size)\n",
    "\n",
    "            assert transposed_mask is not None, \"Masking strategy not found\"\n",
    "            mask = transposed_mask.permute(1, 0)\n",
    "            mask = mask.repeat(batch.size()[0], 1, 1).float().cuda()\n",
    "            loss_mask = ~mask.bool()\n",
    "            batch = batch.permute(0, 2, 1)\n",
    "\n",
    "            assert batch.size() == mask.size() == loss_mask.size(), f'{batch.size()=} {mask.size()=} {loss_mask.size()=}'\n",
    "            \n",
    "            \n",
    "            # assert transposed_mask is not None, \"Masking strategy not found\"\n",
    "            # mask = transposed_mask.permute(0, 2, 1)  # Changed this line\n",
    "            # mask = mask.repeat(batch.size()[0], 1, 1).float().cuda()\n",
    "            # loss_mask = ~mask.bool()\n",
    "            # batch = batch.permute(0, 2, 1)\n",
    "            # \n",
    "            # assert batch.size() == mask.size() == loss_mask.size(), f'{batch.size()=} {mask.size()=} {loss_mask.size()=}'\n",
    "\n",
    "            # back-propagation\n",
    "            optimizer.zero_grad()\n",
    "            X = batch, batch, mask, loss_mask #audio = X[0], cond = X[1], mask = X[2], loss_mask = X[3]\n",
    "            \n",
    "            loss = src_train.training_loss(net, nn.MSELoss(), X, diffusion_hyperparams,\n",
    "                                 only_generate_missing=only_generate_missing)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if n_iter % iters_per_logging == 0:\n",
    "                print(\"iteration: {} \\tloss: {}\".format(n_iter, loss.item()))\n",
    "\n",
    "            # save checkpoint\n",
    "            if n_iter > 0 and n_iter % iters_per_ckpt == 0:\n",
    "                checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "                torch.save({'model_state_dict': net.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict()},\n",
    "                           os.path.join(output_directory, checkpoint_name))\n",
    "                print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "            n_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:28:44.078783200Z",
     "start_time": "2023-12-25T20:28:44.019745200Z"
    }
   },
   "id": "a4f48ddf579b006a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    # # Function to train the model\n",
    "# def train(train_loader, net, optimizer, diffusion_hyperparams, train_config, iters_per_logging, iters_per_ckpt, output_directory):\n",
    "# \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     net = net.to(device)\n",
    "#     n_iter = 0\n",
    "# \n",
    "#     learning_rate = train_config.get('learning_rate', 2e-4)  # Extract learning rate or use default value if not present\n",
    "# \n",
    "#     while n_iter < 2: #train_config['n_iters'] + 1:\n",
    "#         for batch in train_loader:\n",
    "#             input_data, target_data = batch  # Modify this based on your dataset structure\n",
    "#             input_data = input_data.unsqueeze(1)\n",
    "#             target_data = target_data.view(target_data.size(0), -1)  # Modify the dimensions accordingly\n",
    "# \n",
    "# \n",
    "#             transposed_mask = None\n",
    "#             if train_config['masking'] == 'rm':\n",
    "#                 transposed_mask = get_mask_rm(input_data, train_config['missing_k'])\n",
    "#             elif train_config['masking'] == 'mnr':\n",
    "#                 transposed_mask = get_mask_mnr(input_data, train_config['missing_k'])\n",
    "#             elif train_config['masking'] == 'bm':\n",
    "#                 transposed_mask = get_mask_bm(input_data, train_config['missing_k'])\n",
    "# \n",
    "#             # print(f\"Transposed Mask Shape: {transposed_mask.shape}, Input Data Shape: {input_data.shape}, Target Data Shape: {target_data.shape}\")\n",
    "#             mask = transposed_mask.unsqueeze(0)  # Adds a batch dimension\n",
    "#             mask = mask.repeat(input_data.size(0).item(), 1, 1).float().cuda()\n",
    "# \n",
    "#             # mask = mask.repeat(input_data.size()[0], 1, 1).float().cuda()\n",
    "#             loss_mask = ~mask.bool()\n",
    "# \n",
    "# \n",
    "#             # Ensure the sizes are aligned\n",
    "#             assert input_data.size(-1) == mask.size(-1) == loss_mask.size(-1)\n",
    "#             assert input_data.size(1) >= mask.size(1) >= loss_mask.size(1)\n",
    "# \n",
    "#             # print(f\"Batch: {n_iter}, Input Data Size: {input_data.size()}, Mask Size: {mask.size()}, Loss Mask Size: {loss_mask.size()}\")\n",
    "# \n",
    "#             mask = mask.to(device)\n",
    "#             loss_mask = loss_mask.to(device)\n",
    "#             # Update learning rate in the optimizer\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = learning_rate\n",
    "# \n",
    "#             # Back-propagation\n",
    "#             optimizer.zero_grad()\n",
    "#             X = input_data, target_data, mask[:, :, 0:input_data.size(-1)], loss_mask\n",
    "#             loss = training_loss_new(net, nn.MSELoss(), X, diffusion_hyperparams, only_generate_missing=train_config['only_generate_missing'])\n",
    "#             break\n",
    "# \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "# \n",
    "# \n",
    "#             if n_iter % iters_per_logging == 0:\n",
    "#                 print(\"iteration: {} \\tloss: {}\".format(n_iter, loss.item()))\n",
    "# \n",
    "#             # Save checkpoint\n",
    "#             if n_iter > 0 and n_iter % iters_per_ckpt == 0:\n",
    "#                 checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "#                 torch.save({'model_state_dict': net.state_dict(),\n",
    "#                             'optimizer_state_dict': optimizer.state_dict()},\n",
    "#                            os.path.join(output_directory, checkpoint_name))\n",
    "#                 print('Model at iteration %s is saved' % n_iter)\n",
    "# \n",
    "# n_iter += 1\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:28:44.725747500Z",
     "start_time": "2023-12-25T20:28:44.644148300Z"
    }
   },
   "id": "d0c94652695b25f0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory /home/liranc6/ecg/ecg_forecasting/liran_project/results/try/T200_beta00.0001_betaT0.02\n",
      "No valid checkpoint model found, start training from initialization try.\n",
      "training_data.shape=torch.Size([1, 1, 150000, 1])\n",
      "Data loaded\n",
      "n_iter=0\n",
      "i=0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.50 GiB. GPU 0 has a total capacty of 10.75 GiB of which 8.22 GiB is free. Including non-PyTorch memory, this process has 2.53 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 773.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m diffusion_hyperparams\n\u001B[1;32m      2\u001B[0m diffusion_hyperparams \u001B[38;5;241m=\u001B[39m calc_diffusion_hyperparams(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdiffusion_config_SSSDS4)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_directory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_directory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mckpt_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_iters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mn_iters\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43miters_per_ckpt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43miters_per_ckpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43miters_per_logging\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43miters_per_logging\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlearning_rate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43monly_generate_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43monly_generate_missing\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmasking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmasking\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmissing_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config_SSSDS4\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmissing_k\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnet_SSSDS4\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdiffusion_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdiffusion_config_SSSDS4\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdiffusion_hyperparams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcalc_diffusion_hyperparams\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdiffusion_config_SSSDS4\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainset_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrainset_config_SSSDS4\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontext_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabel_window_size\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 184\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(output_directory, ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging, learning_rate, only_generate_missing, masking, missing_k, net, diffusion_config, diffusion_hyperparams, trainset_config, context_size, label_size, **kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    182\u001B[0m X \u001B[38;5;241m=\u001B[39m batch, batch, mask, loss_mask \u001B[38;5;66;03m#audio = X[0], cond = X[1], mask = X[2], loss_mask = X[3]\u001B[39;00m\n\u001B[0;32m--> 184\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43msrc_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMSELoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_hyperparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m                     \u001B[49m\u001B[43monly_generate_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_generate_missing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    188\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/utils/util.py:219\u001B[0m, in \u001B[0;36mtraining_loss\u001B[0;34m(net, loss_fn, X, diffusion_hyperparams, only_generate_missing)\u001B[0m\n\u001B[1;32m    215\u001B[0m transformed_X \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(Alpha_bar[diffusion_steps]) \u001B[38;5;241m*\u001B[39m audio \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m Alpha_bar[diffusion_steps]) \u001B[38;5;241m*\u001B[39m z\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Predict \\epsilon according to \\epsilon_\\theta using the transformed_X, cond, mask, and diffusion_steps. i.e. calling forward\u001B[39;00m\n\u001B[0;32m--> 219\u001B[0m epsilon_theta \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_steps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;66;03m# Compute the loss based on the value of only_generate_missing\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m only_generate_missing \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;66;03m# If only_generate_missing is 1, compute the loss only for the masked elements\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/SSSDS4Imputer.py:208\u001B[0m, in \u001B[0;36mSSSDS4Imputer.forward\u001B[0;34m(self, input_data)\u001B[0m\n\u001B[1;32m    206\u001B[0m x \u001B[38;5;241m=\u001B[39m noise\n\u001B[1;32m    207\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_conv(x)\n\u001B[0;32m--> 208\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresidual_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_steps\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_conv(x)\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/SSSDS4Imputer.py:160\u001B[0m, in \u001B[0;36mResidual_group.forward\u001B[0;34m(self, input_data)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;66;03m# Loop over the residual blocks\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_res_layers):\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;66;03m# Pass the input through the n-th residual block and accumulate the skip connections\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m     h, skip_n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresidual_blocks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_step_embed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m     skip \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m skip_n\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# Return the accumulated skip connections, scaled by the square root of the inverse number of residual layers\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/SSSDS4Imputer.py:91\u001B[0m, in \u001B[0;36mResidual_block.forward\u001B[0;34m(self, input_data)\u001B[0m\n\u001B[1;32m     88\u001B[0m h \u001B[38;5;241m=\u001B[39m h \u001B[38;5;241m+\u001B[39m part_t\n\u001B[1;32m     90\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_layer(h)\n\u001B[0;32m---> 91\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mS41\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m cond \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     94\u001B[0m cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcond_conv(cond)\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/S4Model.py:1199\u001B[0m, in \u001B[0;36mS4Layer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m   1197\u001B[0m     \u001B[38;5;66;03m#x has shape seq, batch, feature\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m)) \u001B[38;5;66;03m#batch, feature, seq (as expected from S4 with transposed=True)\u001B[39;00m\n\u001B[0;32m-> 1199\u001B[0m     xout, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ms4_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#batch, feature, seq\u001B[39;00m\n\u001B[1;32m   1200\u001B[0m     xout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(xout)\n\u001B[1;32m   1201\u001B[0m     xout \u001B[38;5;241m=\u001B[39m xout \u001B[38;5;241m+\u001B[39m x \u001B[38;5;66;03m# skip connection   # batch, feature, seq\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/S4Model.py:1110\u001B[0m, in \u001B[0;36mS4.forward\u001B[0;34m(self, u, **kwargs)\u001B[0m\n\u001B[1;32m   1107\u001B[0m L \u001B[38;5;241m=\u001B[39m u\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;66;03m# Compute SS Kernel\u001B[39;00m\n\u001B[0;32m-> 1110\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mL\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mL\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# (C H L) (B C H L)\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m \u001B[38;5;66;03m# Convolution\u001B[39;00m\n\u001B[1;32m   1113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional:\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/S4Model.py:989\u001B[0m, in \u001B[0;36mHippoSSKernel.forward\u001B[0;34m(self, L)\u001B[0m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, L\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 989\u001B[0m     k, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mL\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m k\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/S4Model.py:686\u001B[0m, in \u001B[0;36mSSKernelNPLR.forward\u001B[0;34m(self, state, rate, L)\u001B[0m\n\u001B[1;32m    684\u001B[0m     r \u001B[38;5;241m=\u001B[39m cauchy_conj(v, z, w)\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 686\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mcauchy_slow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    687\u001B[0m r \u001B[38;5;241m=\u001B[39m r \u001B[38;5;241m*\u001B[39m dt[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, :, \u001B[38;5;28;01mNone\u001B[39;00m]  \u001B[38;5;66;03m# (S+1+R, C+R, H, L)\u001B[39;00m\n\u001B[1;32m    689\u001B[0m \u001B[38;5;66;03m# Low-rank Woodbury correction\u001B[39;00m\n",
      "File \u001B[0;32m~/ecg/ecg_forecasting/SSSD_main/src/imputers/S4Model.py:105\u001B[0m, in \u001B[0;36mcauchy_slow\u001B[0;34m(v, z, w)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcauchy_slow\u001B[39m(v, z, w):\n\u001B[1;32m    100\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03m    v, w: (..., N)\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    z: (..., L)\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m    returns: (..., L)\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m     cauchy_matrix \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[43mz\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m# (... N L)\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39msum(cauchy_matrix, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 12.50 GiB. GPU 0 has a total capacty of 10.75 GiB of which 8.22 GiB is free. Including non-PyTorch memory, this process has 2.53 GiB memory in use. Of the allocated memory 1.55 GiB is allocated by PyTorch, and 773.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "global diffusion_hyperparams\n",
    "diffusion_hyperparams = calc_diffusion_hyperparams(**diffusion_config_SSSDS4)\n",
    "\n",
    "train(\n",
    "    output_directory=train_config_SSSDS4[\"output_directory\"],\n",
    "    ckpt_iter='max',\n",
    "    n_iters=train_config_SSSDS4['n_iters'],\n",
    "    iters_per_ckpt=train_config_SSSDS4['iters_per_ckpt'],\n",
    "    iters_per_logging=train_config_SSSDS4['iters_per_logging'],\n",
    "    learning_rate=train_config_SSSDS4['learning_rate'],\n",
    "    only_generate_missing=train_config_SSSDS4['only_generate_missing'],\n",
    "    masking=train_config_SSSDS4['masking'],\n",
    "    missing_k=train_config_SSSDS4['missing_k'],\n",
    "    net=net_SSSDS4,\n",
    "    diffusion_config=diffusion_config_SSSDS4,\n",
    "    diffusion_hyperparams = calc_diffusion_hyperparams(**diffusion_config_SSSDS4),\n",
    "    trainset_config = trainset_config_SSSDS4,\n",
    "    context_size=context_window_size,\n",
    "    label_size=label_window_size\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T20:29:04.639885300Z",
     "start_time": "2023-12-25T20:28:45.049652200Z"
    }
   },
   "id": "a3f443654cd37673"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(train_loader_SSSDS4, net_SSSDS4, optimizer_SSSDS4, calc_diffusion_hyperparams(**diffusion_config_SSSDS4),\n",
    "      train_config_SSSDS4, iters_per_logging=train_config_SSSDS4['iters_per_logging'],\n",
    "      iters_per_ckpt=train_config_SSSDS4['iters_per_ckpt'],\n",
    "      output_directory=train_config_SSSDS4[\"output_directory\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T20:15:36.528554100Z"
    }
   },
   "id": "d67f6569d653e280"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Test the train function for SSSDSA\n",
    "train(train_loader_SSSDSA, net_SSSDSA, optimizer_SSSDSA, calc_diffusion_hyperparams(**diffusion_config_SSSDSA),\n",
    "      train_config_SSSDSA, iters_per_logging=trainset_config_SSSDSA['iters_per_logging'],\n",
    "      iters_per_ckpt=trainset_config_SSSDSA['iters_per_ckpt'],\n",
    "      output_directory=trainset_config_SSSDSA[\"output_directory\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T20:15:36.530544300Z"
    }
   },
   "id": "968e9bd9dc963ac1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T20:15:36.571995200Z"
    }
   },
   "id": "7d2862732f813c6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
