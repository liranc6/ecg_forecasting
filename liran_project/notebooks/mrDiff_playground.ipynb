{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "from box import Box\n",
    "from pprint import pprint\n",
    "\n",
    "CONFIG_FILENAME = '/home/liranc6/ecg_forecasting/mrDiff/configs/config.yml'\n",
    "\n",
    "assert CONFIG_FILENAME.endswith('.yml')\n",
    "\n",
    "with open(CONFIG_FILENAME, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "ProjectPath = config['project_path']\n",
    "sys.path.append(ProjectPath)\n",
    "\n",
    "# Add the directory containing the exp module to the sys.path\n",
    "exp_module_path = os.path.join(ProjectPath, 'mrDiff')\n",
    "sys.path.append(exp_module_path)\n",
    "\n",
    "from liran_project.mrdiff.src.parser import Args\n",
    "from mrDiff.exp.exp_main import Exp_Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': Box({'project_path': '/home/liranc6/ecg_forecasting', 'general': {'random_seed': 2023, 'evaluate': False, 'tag': None, 'dataset': 'ETTm1', 'features': 'S', 'training_mode': 'ONE', 'interval': 1000}, 'optimization': {'learning_rate': 0.001, 'batch_size': 2048, 'test_batch_size': 32, 'patience': 10, 'weight_decay': 1e-05, 'lradj': '3', 'pct_start': 0.3}, 'hardware': {'num_workers': 0, 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0'}, 'paths': {'checkpoints': './checkpoints/', 'train_data': '/home/liranc6/ecg_forecasting/mrDiff/datasets/SCINet_timeseries/ETT-data/ETT'}, 'data': {'freq': 'h', 'embed': 'timeF', 'cols': [], 'target': -1, 'inverse': False, 'individual': True, 'use_ar_init': False, 'use_residual': True, 'uncertainty': False, 'norm_method': 'z_score', 'normtype': 0}, 'training': {'iterations': {'itr': 1, 'pretrain_epochs': 2, 'train_epochs': 2, 'sample_times': 1}, 'identifiers': {'id_worst': -1, 'focus_variate': -1}, 'sequence': {'seq_len': 336, 'label_len': 336, 'pred_len': 96}, 'model': {'opt_loss_type': 'mse', 'model': 'DDPM', 'base_model': 'Linear', 'u_net_type': 'v0'}, 'analysis': {'vis_MTS_analysis': 0, 'use_window_normalization': True, 'use_future_mixup': True, 'use_X0_in_THiDi': False, 'channel_independence': False}, 'smoothing': {'smoothed_factors': [5, 25, 51]}, 'ode': {'ot_ode': True, 'beta_max': 1.0, 't0': '1e-4', 'T': 0.02, 'nfe': 20}, 'ablation_study': {'ablation_study_F_type': 'Linear', 'beta_schedule': 'cosine', 'beta_dist_alpha': -1, 'ablation_study_masking_type': 'none', 'ablation_study_masking_tau': 0.9}, 'diffusion': {'beta_start': 0.0001, 'beta_end': 0.02, 'diff_steps': 100, 'ddpm_inp_embed': 64, 'ddpm_layers_inp': 10, 'ddpm_dim_diff_steps': 256, 'ddpm_channels_conv': 128, 'ddpm_channels_fusion_I': 256, 'ddpm_layers_I': 5, 'ddpm_layers_II': 10, 'kernel_size': 25, 'dec_channel_nums': 256, 'cond_ddpm_num_layers': 5, 'cond_ddpm_channels_conv': 256}, 'sampler': {'type_sampler': 'dpm', 'parameterization': 'x_start', 'our_ddpm_clip': 100}, 'misc': {'affine': 0, 'subtract_last': 1, 'subtract_short_terms': 0}}}),\n",
      " 'config_filename': '/home/liranc6/ecg_forecasting/mrDiff/configs/config.yml'}\n"
     ]
    }
   ],
   "source": [
    "args = Args(CONFIG_FILENAME)\n",
    "# Now you can use args as needed\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "if args.config['general']['dataset'] == 'ETTm1':\n",
    "    args.config['paths']['datasets_dir'] = '/home/liranc6/ecg_forecasting/mrDiff/datasets/SCINet_timeseries/ETT-data/ETT'\n",
    "    args.config['data']['data'] = 'ETTm1'\n",
    "    args.config['paths']['root_path'] = args.config['paths']['datasets_dir']\n",
    "    args.config['paths']['data_path'] = 'ETTm1.csv'\n",
    "    args.config['data']['num_vars'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics lists\n",
    "mae_, mse_, rmse_, mape_, mspe_, rse_, corr_, nrmse_ = [], [], [], [], [], [], [], []\n",
    "\n",
    "# Ensure the necessary configurations are available\n",
    "random_seed = args.config.get('general', {}).get('random_seed', 2023)\n",
    "tag = args.config.get('general', {}).get('tag', None)\n",
    "dataset = args.config.get('general', {}).get('dataset', 'default_dataset')\n",
    "features = args.config.get('general', {}).get('features', 'S')\n",
    "\n",
    "learning_rate = args.config.get('optimization', {}).get('learning_rate', 0.001)\n",
    "batch_size = args.config.get('optimization', {}).get('batch_size', 64)\n",
    "\n",
    "seq_len = args.config.get('training', {}).get('sequence', {}).get('seq_len', 336)\n",
    "label_len = args.config.get('training', {}).get('sequence', {}).get('label_len', 336)\n",
    "model = args.config.get('training', {}).get('model', {}).get('model', 'default_model')\n",
    "pred_len = args.config.get('training', {}).get('sequence', {}).get('pred_len', 96)\n",
    "iterations = args.config.get('training', {}).get('iterations', {}).get('itr', 1)\n",
    "\n",
    "inverse = args.config.get('data', {}).get('inverse', False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : DDPM_ETTm1_ftS_sl336_ll336_pl96_lr0.001_bs2048_invFalse_itr0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 34129\n",
      "val 11425\n",
      "test 11425\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> nrmse:\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, std:\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mmean(nrmse_), np\u001b[38;5;241m.\u001b[39mstd(nrmse_)))\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m: (np\u001b[38;5;241m.\u001b[39mmean(mae_), np\u001b[38;5;241m.\u001b[39mstd(mae_)),\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m: (np\u001b[38;5;241m.\u001b[39mmean(mse_), np\u001b[38;5;241m.\u001b[39mstd(mse_)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: (np\u001b[38;5;241m.\u001b[39mmean(nrmse_), np\u001b[38;5;241m.\u001b[39mstd(nrmse_))\n\u001b[1;32m     65\u001b[0m     }\n\u001b[0;32m---> 67\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlabel_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(iterations, random_seed, model, dataset, features, seq_len, label_len, pred_len, learning_rate, batch_size, inverse, tag, args)\u001b[0m\n\u001b[1;32m     29\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp_Main(args)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m mae, mse, rmse, mape, mspe, rse, corr, nrmse \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mtest(setting, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ecg_forecasting/mrDiff/exp/exp_main.py:217\u001b[0m, in \u001b[0;36mExp_Main.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    214\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    216\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 217\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/ecg_forecasting/mrDiff/models_diffusion/DDPM.py:287\u001b[0m, in \u001b[0;36mModel.train_forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, return_mean, meta_weights, train_val)\u001b[0m\n\u001b[1;32m    285\u001b[0m     future_trend_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrev(future_trend_i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenorm\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39manalysis\u001b[38;5;241m.\u001b[39muse_window_normalization \u001b[38;5;28;01melse\u001b[39;00m future_trend_i\n\u001b[1;32m    286\u001b[0m     past_trend_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrev(past_trend_i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenorm\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39manalysis\u001b[38;5;241m.\u001b[39muse_window_normalization \u001b[38;5;28;01melse\u001b[39;00m past_trend_i\n\u001b[0;32m--> 287\u001b[0m     linear_guess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_trend_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_history_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_trend_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    288\u001b[0m     linear_guess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrev(linear_guess, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_norm\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39manalysis\u001b[38;5;241m.\u001b[39muse_window_normalization \u001b[38;5;28;01melse\u001b[39;00m linear_guess\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ecg_forecasting/mrDiff/models_diffusion/DDPM.py:103\u001b[0m, in \u001b[0;36mBaseMapping.test_forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     trend_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([trend_init\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),trend_init\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len],dtype\u001b[38;5;241m=\u001b[39mtrend_init\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(trend_init\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels):\n\u001b[0;32m--> 103\u001b[0m         trend_output[:,i,:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear_Trend[i](\u001b[43mtrend_init\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     trend_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear_Trend(trend_init)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "def run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                    label_len, pred_len, learning_rate, batch_size, inverse, tag, args):\n",
    "    mae_ = []\n",
    "    mse_ = []\n",
    "    rmse_ = []\n",
    "    mape_ = []\n",
    "    mspe_ = []\n",
    "    rse_ = []\n",
    "    corr_ = []\n",
    "    nrmse_ = []\n",
    "\n",
    "    for ii in range(iterations):\n",
    "        # setting record of experiments\n",
    "\n",
    "        # random seed\n",
    "        fix_seed = ii if iterations > 1 else random_seed\n",
    "\n",
    "        random.seed(fix_seed)\n",
    "        torch.manual_seed(fix_seed)\n",
    "        np.random.seed(fix_seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        setting = f\"{model}_{dataset}_ft{features}_sl{seq_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{ii}\"\n",
    "        if tag is not None:\n",
    "            setting += f\"_{tag}\"\n",
    "\n",
    "        exp = Exp_Main(args)\n",
    "\n",
    "        print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        exp.train(setting)\n",
    "\n",
    "        print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "        mae, mse, rmse, mape, mspe, rse, corr, nrmse = exp.test(setting, test=1)\n",
    "\n",
    "        mae_.append(mae)\n",
    "        mse_.append(mse)\n",
    "        rmse_.append(rmse)\n",
    "        mape_.append(mape)\n",
    "        mspe_.append(mspe)\n",
    "        rse_.append(rse)\n",
    "        corr_.append(corr)\n",
    "        nrmse_.append(nrmse)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print('Final mean normed: ')\n",
    "    print('> mae:{:.4f}, std:{:.4f}'.format(np.mean(mae_), np.std(mae_)))\n",
    "    print('> mse:{:.4f}, std:{:.4f}'.format(np.mean(mse_), np.std(mse_)))\n",
    "    print('> rmse:{:.4f}, std:{:.4f}'.format(np.mean(rmse_), np.std(rmse_)))\n",
    "    print('> mape:{:.4f}, std:{:.4f}'.format(np.mean(mape_), np.std(mape_)))\n",
    "    print('> rse:{:.4f}, std:{:.4f}'.format(np.mean(rse_), np.std(rse_)))\n",
    "    print('> corr:{:.4f}, std:{:.4f}'.format(np.mean(corr_), np.std(corr_)))\n",
    "    print('> nrmse:{:.4f}, std:{:.4f}'.format(np.mean(nrmse_), np.std(nrmse_)))\n",
    "\n",
    "    return {\n",
    "        'mae': (np.mean(mae_), np.std(mae_)),\n",
    "        'mse': (np.mean(mse_), np.std(mse_)),\n",
    "        'rmse': (np.mean(rmse_), np.std(rmse_)),\n",
    "        'mape': (np.mean(mape_), np.std(mape_)),\n",
    "        'rse': (np.mean(rse_), np.std(rse_)),\n",
    "        'corr': (np.mean(corr_), np.std(corr_)),\n",
    "        'nrmse': (np.mean(nrmse_), np.std(nrmse_))\n",
    "    }\n",
    "    \n",
    "results = run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                            label_len, pred_len, learning_rate, batch_size, inverse, tag, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
