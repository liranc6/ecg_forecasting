{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "from box import Box\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "CONFIG_FILENAME = '/home/liranc6/ecg_forecasting/liran_project/mrdiff/src/config_ecg.yml'\n",
    "\n",
    "assert CONFIG_FILENAME.endswith('.yml')\n",
    "\n",
    "with open(CONFIG_FILENAME, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "ProjectPath = config['project_path']\n",
    "sys.path.append(ProjectPath)\n",
    "\n",
    "from liran_project.mrdiff.src.parser import parse_args\n",
    "from liran_project.utils.dataset_loader import SingleLeadECGDatasetCrops_mrDiff as DataSet\n",
    "from liran_project.utils.util import ecg_signal_difference\n",
    "import liran_project.mrdiff.exp_main\n",
    "# from liran_project.mrdiff.exp_main import Exp_Main\n",
    "\n",
    "# Add the directory containing the exp module to the sys.path\n",
    "exp_module_path = os.path.join(ProjectPath, 'mrDiff')\n",
    "sys.path.append(exp_module_path)\n",
    "\n",
    "# from mrDiff.exp.exp_main import Exp_Main\n",
    "from mrDiff.data_process.etth_dataloader import Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom, Dataset_Wind, Dataset_Caiso, Dataset_Production, Dataset_Caiso_M, Dataset_Production_M\n",
    "from mrDiff.data_process.financial_dataloader import DatasetH\n",
    "from mrDiff.data_process.forecast_dataloader import ForecastDataset\n",
    "from mrDiff.exp.exp_basic import Exp_Basic\n",
    "from mrDiff.models_diffusion import DDPM\n",
    "from mrDiff.utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from mrDiff.utils.metrics import metric\n",
    "\n",
    "from liran_project.mrdiff.src.parser import Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': Box({'project_path': '/home/liranc6/ecg_forecasting', 'tqdm': 'notebook', 'wandb': {'entity': 'liranc6', 'mode': 'disabled', 'project': 'mrdiff', 'resume': 'None', 'run_name': 'first_run_ever', 'id': 'None', 'save_code': True, 'resume_from': 'None'}, 'general': {'random_seed': 42, 'evaluate': False, 'tag': None, 'dataset': 'icentia11k', 'features': 'S', 'training_mode': 'ONE', 'interval': 1000}, 'optimization': {'learning_rate': 0.001, 'batch_size': 8, 'test_batch_size': 8, 'patience': 10, 'weight_decay': 1e-05, 'lradj': '3', 'pct_start': 0.3}, 'hardware': {'print_gpu_memory_usage': True, 'num_workers': 0, 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0', 'device_ids': [0]}, 'paths': {'train_data': '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/train/p0_to_p32.h5', 'val_data': '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/val/p33_to_p39.h5', 'test_data': '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/test/p40_to_p46.h5', 'output_dir': '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff', 'checkpoints': '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff'}, 'data': {'fs': 250, 'freq': 'h', 'embed': 'timeF', 'cols': [], 'target': -1, 'inverse': False, 'individual': True, 'use_ar_init': False, 'use_residual': True, 'uncertainty': False, 'norm_method': 'None', 'normtype': 0, 'num_vars': 1}, 'training': {'logging': {'sample': False, 'log_interval': 100, 'save_interval': 1, 'save_best': True, 'save_dir': '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff'}, 'patients': {'start_patient': 0, 'end_patient': 1}, 'iterations': {'itr': 2, 'pretrain_epochs': 0, 'train_epochs': 2, 'sample_times': 1}, 'identifiers': {'id_worst': -1, 'focus_variate': -1}, 'sequence': {'context_len': 10, 'seq_len': 150000, 'label_len': 105000, 'pred_len': 30000}, 'model_info': {'opt_loss_type': 'mse', 'model': 'DDPM', 'base_model': 'Linear', 'u_net_type': 'v0'}, 'analysis': {'vis_MTS_analysis': 0, 'use_window_normalization': True, 'use_future_mixup': True, 'use_X0_in_THiDi': False, 'channel_independence': False}, 'smoothing': {'smoothed_factors': [5, 25, 51]}, 'ode': {'ot_ode': True, 'beta_max': 1.0, 't0': '1e-4', 'T': 0.02, 'nfe': 20}, 'ablation_study': {'ablation_study_F_type': 'Linear', 'beta_schedule': 'cosine', 'beta_dist_alpha': -1, 'ablation_study_masking_type': 'none', 'ablation_study_masking_tau': 0.9}, 'diffusion': {'beta_start': 0.0001, 'beta_end': 0.02, 'diff_steps': 100, 'ddpm_inp_embed': 64, 'ddpm_layers_inp': 10, 'ddpm_dim_diff_steps': 256, 'ddpm_channels_conv': 128, 'ddpm_channels_fusion_I': 256, 'ddpm_layers_I': 5, 'ddpm_layers_II': 10, 'kernel_size': 25, 'dec_channel_nums': 256, 'cond_ddpm_num_layers': 5, 'cond_ddpm_channels_conv': 256}, 'sampler': {'type_sampler': 'dpm', 'parameterization': 'x_start', 'our_ddpm_clip': 100}, 'misc': {'affine': 0, 'subtract_last': 0, 'subtract_short_terms': 0}}, 'testing': {'patients': {'start_patient': 0, 'end_patient': 1}}, 'validation': {'patients': {'start_patient': 0, 'end_patient': 1}}, 'use_gpu': True}),\n",
      " 'config_filename': '/home/liranc6/ecg_forecasting/liran_project/mrdiff/src/config_ecg.yml'}\n"
     ]
    }
   ],
   "source": [
    "args = Args(CONFIG_FILENAME)\n",
    "\n",
    "# Now you can use args as needed\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Box object to dictionary\n",
    "config_dict = args.config.to_dict()\n",
    "\n",
    "# Access the configuration values using dictionary syntax\n",
    "random_seed = config_dict['general']['random_seed']\n",
    "tag = config_dict['general']['tag']\n",
    "dataset = config_dict['general']['dataset']\n",
    "features = config_dict['general']['features']\n",
    "\n",
    "learning_rate = config_dict['optimization']['learning_rate']\n",
    "batch_size = config_dict['optimization']['batch_size']\n",
    "\n",
    "context_len = config_dict['training']['sequence']['context_len']\n",
    "label_len = config_dict['training']['sequence']['label_len']\n",
    "model = config_dict['training']['model_info']['model']\n",
    "pred_len = config_dict['training']['sequence']['pred_len']\n",
    "iterations = config_dict['training']['iterations']['itr']\n",
    "\n",
    "inverse = config_dict['data']['inverse']\n",
    "    \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False\n",
    "torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'disabled', 'project': 'mrdiff', 'save_code': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb\n",
    "wandb_init_config ={\n",
    "        \"mode\": args.wandb.mode,\n",
    "        \"project\": args.wandb.project,\n",
    "        \"save_code\": args.wandb.save_code,\n",
    "    }\n",
    "wandb_init_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New wandb run id: adqtvt5n\n"
     ]
    }
   ],
   "source": [
    "if args.wandb.resume != \"None\":\n",
    "    wandb_init_config.update({\n",
    "                            \"id\": args.wandb.resume,\n",
    "                            \"resume\": args.wandb.resume\n",
    "                            })\n",
    "    \n",
    "    if args.wandb.resume_from != \"None\":\n",
    "        wandb_init_config[\"config\"] = args.wandb.resume_from\n",
    "        \n",
    "    run = wandb.init(**wandb_init_config)\n",
    "    print(f\"Resuming wandb run id: {wandb.run.id}\")\n",
    "    \n",
    "    def log_config_diffs(old_config, new_config, step):\n",
    "        diffs = {}\n",
    "        for key in new_config:\n",
    "            if key not in old_config or old_config[key] != new_config[key]:\n",
    "                diffs[key] = {'old': old_config.get(key), 'new': new_config[key]}\n",
    "    \n",
    "        if diffs:\n",
    "            note = f\"Config changes at step {step}:\\n\"\n",
    "            for key, value in diffs.items():\n",
    "                note += f\"{key}: {value['old']} -> {value['new']}\\n\"\n",
    "            wandb.run.notes = (wandb.run.notes or \"\") + note + \"\\n\\nAdditional information added later:\\n\"\n",
    "    \n",
    "    old_config = wandb.config.copy()\n",
    "    wandb.config.update(args)\n",
    "    new_config = wandb.config.copy()\n",
    "    log_config_diffs(old_config, new_config, step=\"update_args\")\n",
    "            \n",
    "else:\n",
    "    wandb.init(**wandb_init_config, config=args, )\n",
    "    print(f\"New wandb run id: {wandb.run.id}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "iteration = 1\n",
    "# setting\n",
    "setting = f\"{model}_{dataset}_ft{features}_sl{context_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iteration}\"\n",
    "\n",
    "if tag is not None:\n",
    "    setting += f\"_{tag}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'liran_project.mrdiff.exp_main' from '/home/liranc6/ecg_forecasting/liran_project/mrdiff/exp_main.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(liran_project.mrdiff.exp_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "exp = liran_project.mrdiff.exp_main.Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 00001: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]\n",
      "Reading 00033: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "exp.read_data('train')\n",
    "exp.read_data('val')\n",
    "# exp.read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1404.24s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mchuck1                  \u001b[m  Sat Oct  5 01:59:23 2024  \u001b[1m\u001b[30m535.161.08\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A100-SXM4-80GB\u001b[m |\u001b[31m 22°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[1m\u001b[33m18802\u001b[m / \u001b[33m81920\u001b[m MB | \u001b[1m\u001b[30mliranc6\u001b[m(\u001b[33m18794M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(liran_project.mrdiff.exp_main)\n",
    "\n",
    "# Assuming `exp` is an existing instance of `Exp_Main`\n",
    "exp.__class__ = liran_project.mrdiff.exp_main.Exp_Main\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "! gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-05 01:59:33 2400857:2400857 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : DDPM_icentia11k_ftS_sl10_ll105000_pl30000_lr0.001_bs8_invFalse_itr1>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Saving model to /home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105000_pl30000_lr0.001_bs8_invFalse_itr1/05_10_2024_0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs_pbar:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "epochs_pbar:   0%|          | 0/2 [00:02<?, ?it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 17.83 GB\n",
      "  Allocated Memory: 17.70 GB\n",
      "  Free Memory: 0.13 GB\n",
      "epoch: 0, i: 0{'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 17.826171875, 'allocated_memory_gb': 17.699163913726807, 'free_memory_gb': 0.12700796127319336}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 17.83 GB\n",
      "  Allocated Memory: 17.72 GB\n",
      "  Free Memory: 0.11 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 17.83203125, 'allocated_memory_gb': 17.722947120666504, 'free_memory_gb': 0.1090841293334961}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.71 GB\n",
      "  Allocated Memory: 32.48 GB\n",
      "  Free Memory: 0.23 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.705078125, 'allocated_memory_gb': 32.47595405578613, 'free_memory_gb': 0.2291240692138672}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.58 GB\n",
      "  Allocated Memory: 47.23 GB\n",
      "  Free Memory: 0.35 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.576171875, 'allocated_memory_gb': 47.22806692123413, 'free_memory_gb': 0.34810495376586914}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.45 GB\n",
      "  Allocated Memory: 61.98 GB\n",
      "  Free Memory: 0.47 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.44921875, 'allocated_memory_gb': 61.97883892059326, 'free_memory_gb': 0.4703798294067383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "epochs_pbar:   0%|          | 0/2 [18:20<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 70.55 GB\n",
      "  Allocated Memory: 17.84 GB\n",
      "  Free Memory: 52.70 GB\n",
      "epoch: 0, i: 1{'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 70.546875, 'allocated_memory_gb': 17.844829559326172, 'free_memory_gb': 52.70204544067383}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 70.55 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 52.73 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 70.546875, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 52.72979784011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 77.87 GB\n",
      "  Allocated Memory: 17.82 GB\n",
      "  Free Memory: 60.05 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 77.87109375, 'allocated_memory_gb': 17.817077159881592, 'free_memory_gb': 60.05401659011841}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 32.74 GB\n",
      "  Allocated Memory: 32.57 GB\n",
      "  Free Memory: 0.17 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 32.736328125, 'allocated_memory_gb': 32.57077693939209, 'free_memory_gb': 0.16555118560791016}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 47.61 GB\n",
      "  Allocated Memory: 47.32 GB\n",
      "  Free Memory: 0.28 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 47.607421875, 'allocated_memory_gb': 47.32288980484009, 'free_memory_gb': 0.2845320701599121}\n",
      "GPU cuda:0 (NVIDIA A100-SXM4-80GB) Memory Usage:\n",
      "  Total Memory: 79.15 GB\n",
      "  Reserved Memory: 62.48 GB\n",
      "  Allocated Memory: 62.07 GB\n",
      "  Free Memory: 0.41 GB\n",
      "after 'x_k = self.q_sample(x_start=x, t=t, noise=noise){'gpu_name': 'NVIDIA A100-SXM4-80GB', 'total_memory_gb': 79.1510009765625, 'reserved_memory_gb': 62.478515625, 'allocated_memory_gb': 62.07276725769043, 'free_memory_gb': 0.4057483673095703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loader_pbar:  36%|███▋      | 12/33 [18:41<32:42, 93.44s/it]\n",
      "epochs_pbar:   0%|          | 0/2 [18:41<?, ?it/s]\n",
      "STAGE:2024-10-05 02:18:15 2400857:2400857 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-05 02:18:15 2400857:2400857 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "[W collection.cpp:936] Warning: Failed to recover relationship between all profiler and kineto events: 94633 vs. 0  reassociated. (function reassociate)\n"
     ]
    }
   ],
   "source": [
    "print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "try:\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            # torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        exp.train(setting)\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during training: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us      18.31 Mb      18.31 Mb      78.08 Gb      78.08 Gb          1554  \n",
      "                                        cudaMemcpyAsync         1.23%       6.926ms         1.23%       6.926ms     266.385us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.455ms         0.51%       1.455ms      80.833us           0 b           0 b           0 b           0 b            18  \n",
      "                                  cudaStreamSynchronize        21.49%     121.087ms        21.49%     121.087ms       6.727ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            18  \n",
      "                                       cudaLaunchKernel        30.12%     169.752ms        30.12%     169.752ms     169.922us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           999  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      85.000us         0.03%      85.000us       6.071us           0 b           0 b           0 b           0 b            14  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      58.000us         0.02%      58.000us      11.600us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.00%       5.000us       1.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.00%      10.000us       2.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      31.349ms        10.92%      31.349ms     265.669us           0 b           0 b           0 b           0 b           118  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 563.570ms\n",
      "Self CUDA time total: 287.026ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `prof` is your profiler object\n",
    "key_averages = prof.key_averages()\n",
    "\n",
    "# Filter to include only CUDA operations\n",
    "cuda_operations = [item for item in key_averages if 'cuda' in item.key]\n",
    "\n",
    "# Print the filtered table\n",
    "print(key_averages.table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CUDA Memory Usage: 83842590208 bytes\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us      18.31 Mb      18.31 Mb      78.08 Gb      78.08 Gb          1554  \n",
      "                                        cudaMemcpyAsync         1.23%       6.926ms         1.23%       6.926ms     266.385us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.455ms         0.51%       1.455ms      80.833us           0 b           0 b           0 b           0 b            18  \n",
      "                                  cudaStreamSynchronize        21.49%     121.087ms        21.49%     121.087ms       6.727ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            18  \n",
      "                                       cudaLaunchKernel        30.12%     169.752ms        30.12%     169.752ms     169.922us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           999  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      85.000us         0.03%      85.000us       6.071us           0 b           0 b           0 b           0 b            14  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      58.000us         0.02%      58.000us      11.600us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.00%       5.000us       1.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.00%      10.000us       2.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      31.349ms        10.92%      31.349ms     265.669us           0 b           0 b           0 b           0 b           118  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 563.570ms\n",
      "Self CUDA time total: 287.026ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# Assuming `prof` is your profiler object\n",
    "key_averages = prof.key_averages()\n",
    "\n",
    "# Total CUDA memory usage\n",
    "total_cuda_memory_usage = sum(item.self_cuda_memory_usage for item in key_averages)\n",
    "print(f\"Total CUDA Memory Usage: {total_cuda_memory_usage} bytes\")\n",
    "\n",
    "# Memory usage by function\n",
    "print(key_averages.table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us      18.31 Mb      18.31 Mb      78.08 Gb      78.08 Gb          1554  \n",
      "                                        cudaMemcpyAsync         1.23%       6.926ms         1.23%       6.926ms     266.385us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.455ms         0.51%       1.455ms      80.833us           0 b           0 b           0 b           0 b            18  \n",
      "                                  cudaStreamSynchronize        21.49%     121.087ms        21.49%     121.087ms       6.727ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            18  \n",
      "                                       cudaLaunchKernel        30.12%     169.752ms        30.12%     169.752ms     169.922us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           999  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      85.000us         0.03%      85.000us       6.071us           0 b           0 b           0 b           0 b            14  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      58.000us         0.02%      58.000us      11.600us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.00%       5.000us       1.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.00%      10.000us       2.000us           0 b           0 b           0 b           0 b             5  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      31.349ms        10.92%      31.349ms     265.669us           0 b           0 b           0 b           0 b           118  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 563.570ms\n",
      "Self CUDA time total: 287.026ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 25           |        cudaMalloc retries: 26        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  66618 KiB |  80015 MiB | 104888 MiB | 104823 MiB |\n",
      "|       from large pool |  22384 KiB |  79971 MiB | 104827 MiB | 104805 MiB |\n",
      "|       from small pool |  44234 KiB |     44 MiB |     60 MiB |     17 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  66618 KiB |  80015 MiB | 104888 MiB | 104823 MiB |\n",
      "|       from large pool |  22384 KiB |  79971 MiB | 104827 MiB | 104805 MiB |\n",
      "|       from small pool |  44234 KiB |     44 MiB |     60 MiB |     17 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  66557 KiB |  79929 MiB | 104770 MiB | 104705 MiB |\n",
      "|       from large pool |  22382 KiB |  79886 MiB | 104709 MiB | 104687 MiB |\n",
      "|       from small pool |  44175 KiB |     44 MiB |     60 MiB |     17 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  80472 MiB |  80472 MiB |  80740 MiB | 274432 KiB |\n",
      "|       from large pool |  80426 MiB |  80426 MiB |  80692 MiB | 272384 KiB |\n",
      "|       from small pool |     46 MiB |     48 MiB |     48 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  21446 KiB |   1225 MiB |   4167 MiB |   4146 MiB |\n",
      "|       from large pool |  18576 KiB |   1222 MiB |   4118 MiB |   4100 MiB |\n",
      "|       from small pool |   2870 KiB |      4 MiB |     49 MiB |     46 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     743    |    1297    |    1590    |     847    |\n",
      "|       from large pool |       9    |     371    |     573    |     564    |\n",
      "|       from small pool |     734    |     926    |    1017    |     283    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     743    |    1297    |    1590    |     847    |\n",
      "|       from large pool |       9    |     371    |     573    |     564    |\n",
      "|       from small pool |     734    |     926    |    1017    |     283    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     349    |     351    |     352    |       3    |\n",
      "|       from large pool |     326    |     327    |     328    |       2    |\n",
      "|       from small pool |      23    |      24    |      24    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      50    |     340    |     327    |\n",
      "|       from large pool |       2    |      38    |     173    |     171    |\n",
      "|       from small pool |      11    |      20    |     167    |     156    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.model_start_training_time = \"02_10_2024_135344\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>testing : DDPM_icentia11k_ftS_sl10_ll105000_pl45000_lr0.001_bs8_invFalse_itr1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "loading model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105000_pl45000_lr0.001_bs8_invFalse_itr1/05_10_2024_0030/checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ecg_forecasting/liran_project/mrdiff/exp_main.py:386\u001b[0m, in \u001b[0;36mExp_Main.test\u001b[0;34m(self, setting, time_path, test, visualize)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    385\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mcheckpoints, setting, time_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    388\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m trues \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105000_pl45000_lr0.001_bs8_invFalse_itr1/05_10_2024_0030/checkpoint.pth'"
     ]
    }
   ],
   "source": [
    "print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "exp.test(setting, test=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DDPM_icentia11k_ftS_sl1_ll1500_pl1500_lr0.001_bs8_invFalse_itr0>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading 00001: 100%|██████████| 2/2 [00:24<00:00, 12.28s/it]\n",
      "Reading 00001: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n",
      "Reading 00042: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp_Main(args)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m exp\u001b[38;5;241m.\u001b[39mtest(setting, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 180\u001b[0m, in \u001b[0;36mExp_Main.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    177\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    178\u001b[0m results \u001b[38;5;241m=\u001b[39m Metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_x, batch_y, batch_x_mark, batch_y_mark) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    183\u001b[0m         batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ecg_forecasting/liran_project/utils/dataset_loader.py:216\u001b[0m, in \u001b[0;36mSingleLeadECGDatasetCrops_mrDiff.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    212\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m h5_file[str_dataset_idx]\n\u001b[1;32m    214\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size, \u001b[38;5;28mlen\u001b[39m(dataset)) \n\u001b[0;32m--> 216\u001b[0m     to_cache \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# This is a list of windows, each window is a numpy array with shape (window_size) if no RR data,\u001b[39;00m\n\u001b[1;32m    217\u001b[0m                                             \u001b[38;5;66;03m# or (2, window_size) if there is RR data\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_to_cache(to_cache, idx, advance)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.10/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    # setting record of experiments\n",
    "\n",
    "    # random seed\n",
    "    \n",
    "    # setting\n",
    "    setting = f\"{model}_{dataset}_ft{features}_sl{context_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iteration}\"\n",
    "    \n",
    "    if tag is not None:\n",
    "        setting += f\"_{tag}\"\n",
    "\n",
    "    exp = Exp_Main(args)\n",
    "\n",
    "    print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    exp.train(setting)\n",
    "\n",
    "    print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    exp.test(setting, test=1)\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.config.general.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                    label_len, pred_len, learning_rate, batch_size, inverse, tag, args):\n",
    "    mae_ = []\n",
    "    mse_ = []\n",
    "    rmse_ = []\n",
    "    mape_ = []\n",
    "    mspe_ = []\n",
    "    rse_ = []\n",
    "    corr_ = []\n",
    "    nrmse_ = []\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        # setting record of experiments\n",
    "\n",
    "        # random seed\n",
    "        fix_seed = iter if iterations > 1 else random_seed\n",
    "\n",
    "        random.seed(fix_seed)\n",
    "        torch.manual_seed(fix_seed)\n",
    "        np.random.seed(fix_seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        setting = f\"{model}_{dataset}_ft{features}_sl{seq_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iter}\"\n",
    "        if tag is not None:\n",
    "            setting += f\"_{tag}\"\n",
    "\n",
    "        exp = Exp_Main(args)\n",
    "\n",
    "        print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        exp.train(setting)\n",
    "\n",
    "        print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "        mae, mse, rmse, mape, mspe, rse, corr, nrmse = exp.test(setting, test=1)\n",
    "\n",
    "        mae_.append(mae)\n",
    "        mse_.append(mse)\n",
    "        rmse_.append(rmse)\n",
    "        mape_.append(mape)\n",
    "        mspe_.append(mspe)\n",
    "        rse_.append(rse)\n",
    "        corr_.append(corr)\n",
    "        nrmse_.append(nrmse)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print('Final mean normed: ')\n",
    "    print('> mae:{:.4f}, std:{:.4f}'.format(np.mean(mae_), np.std(mae_)))\n",
    "    print('> mse:{:.4f}, std:{:.4f}'.format(np.mean(mse_), np.std(mse_)))\n",
    "    print('> rmse:{:.4f}, std:{:.4f}'.format(np.mean(rmse_), np.std(rmse_)))\n",
    "    print('> mape:{:.4f}, std:{:.4f}'.format(np.mean(mape_), np.std(mape_)))\n",
    "    print('> rse:{:.4f}, std:{:.4f}'.format(np.mean(rse_), np.std(rse_)))\n",
    "    print('> corr:{:.4f}, std:{:.4f}'.format(np.mean(corr_), np.std(corr_)))\n",
    "    print('> nrmse:{:.4f}, std:{:.4f}'.format(np.mean(nrmse_), np.std(nrmse_)))\n",
    "\n",
    "    return {\n",
    "        'mae': (np.mean(mae_), np.std(mae_)),\n",
    "        'mse': (np.mean(mse_), np.std(mse_)),\n",
    "        'rmse': (np.mean(rmse_), np.std(rmse_)),\n",
    "        'mape': (np.mean(mape_), np.std(mape_)),\n",
    "        'rse': (np.mean(rse_), np.std(rse_)),\n",
    "        'corr': (np.mean(corr_), np.std(corr_)),\n",
    "        'nrmse': (np.mean(nrmse_), np.std(nrmse_))\n",
    "    }\n",
    "    \n",
    "results = run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                            label_len, pred_len, learning_rate, batch_size, inverse, tag, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
