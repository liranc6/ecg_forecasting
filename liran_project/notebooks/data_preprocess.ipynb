{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3220d5c3d12660",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing for ICENTIA11K Dataset\n",
    "\n",
    "After exploring the dataset using the notebook [Explore ICENTIA11K Dataset Notebook](./explore_ICENTIA11K_dataset.ipynb), the next step is to preprocess the data. As a reminder, the main sample/label in the dataset is represented by the `p_signal`, which is in the format of `np.ndarray`.\n",
    "\n",
    "## Preprocessing Steps:\n",
    "\n",
    "1. **Transform to Tensor:**\n",
    "   - Convert the `np.ndarray` samples to tensors.\n",
    "\n",
    "2. **Divide into Features and Labels:**\n",
    "   - Split the tensor into features (X) and labels (y). The initial split ratio will be 9:1.\n",
    "\n",
    "3. **Adjusting Sample Length:**\n",
    "   - To maintain consistency, fix the length of each sample. Split each file into as many examples as possible, ignoring any remainder. It's important to note that this length can be adjusted as a parameter in the script.\n",
    "\n",
    "4. **Saving Data as Tensors:**\n",
    "   - For time efficiency, save the preprocessed data as tensors. This helps in quick loading and further analysis without the need for repetitive preprocessing.\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "To implement these steps, refer to the code in this notebook. Additionally, remember that the sample length is a parameter that can be adjusted based on experimentation. It's worth noting that the script will handle the splitting of files into fixed-length examples during runtime, without creating new files.\n",
    "\n",
    "This preprocessing stage ensures that the data is in a suitable format for training machine learning models. Adjusting the sample length allows for flexibility in model training and experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f7ff234b84330",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use [SSSD-main/.../timeseries_utils.py](SSSD-main/docs/instructions/PTB-XL/clinical_ts/timeseries_utils.py) functions to preprocess:\n",
    "\n",
    "1. ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "2. class TimeseriesDatasetCrops(torch.utils.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72ea9d4fccfe8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T13:34:51.366933Z",
     "start_time": "2024-02-14T13:34:49.150582200Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liranc6/miniconda3/envs/ecg/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sys.path.append('..')  # Add the parent directory to the sys.path\n",
    "\n",
    "import utils.data_preparation as data_preparation\n",
    "\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "# ! gpustat\n",
    "\n",
    "# data_path = '/mnt/qnap/liranc6/data/'\n",
    "# subset_data_dir = \"/mnt/qnap/liranc6/data/icentia11k-continuous-ecg_normal_sinus_subset/\" #patients 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c7a57b60095ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T13:35:04.025998200Z",
     "start_time": "2024-02-14T13:35:03.967156100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dir_tree(path, level=0):\n",
    "    for root, _, files in os.walk(path):\n",
    "        indent = ' ' * level\n",
    "        print(f'{indent}{root}')\n",
    "        for file in files:\n",
    "            print(f'{indent}└─{file}')\n",
    "        level += 1\n",
    "        \n",
    "\n",
    "# dir_tree(subset_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867a1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "from utils.data_preparation import find_beat_indices\n",
    "\n",
    "def extract_and_save_p_signal_to_HDF5(input_dir, output_file, with_R_beats=False):\n",
    "    \"\"\"\n",
    "    Read ECG signals and save the p_signal data as NumPy arrays in an HDF5 file\n",
    "    while preserving the directory hierarchy.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir: Directory containing ECG data.\n",
    "    - output_file: The HDF5 file to save the p_signal data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    num_files = sum([1 for _ in os.walk(input_dir)])\n",
    "    # Open the HDF5 file in write mode\n",
    "    with h5py.File(output_file, 'w') as h5_file:\n",
    "        # Traverse the input directory to find data files\n",
    "        for root, _, files in tqdm(os.walk(input_dir), desc=\"Processing files\", total=num_files, unit=\"file\"):\n",
    "            for file in files:\n",
    "                # Check if the file is a data file (ends with .dat)\n",
    "                if file.endswith('.dat'):\n",
    "                    record_name = os.path.splitext(file)[0]\n",
    "\n",
    "                    # Get the relative path from the input directory\n",
    "                    relative_path = os.path.relpath(root, input_dir)\n",
    "                    dataset_name = os.path.join(relative_path, f\"{record_name}_p_signal\")\n",
    "\n",
    "                    # Read the signal using wfdb\n",
    "                    filename = os.path.join(root, record_name)\n",
    "                    \n",
    "                    signals, fields = wfdb.rdsamp(filename)\n",
    "\n",
    "                    # Define beat types for annotation plotting\n",
    "                    beat_types = ['N', 'Q', '+', 'V', 'S']\n",
    "\n",
    "                    if with_R_beats:\n",
    "                        # Read the annotations\n",
    "                        ann = wfdb.rdann(filename, 'atr')\n",
    "                        indices = [item for sublist in find_beat_indices(ann, beat_types).values() for item in sublist]\n",
    "                        indices = np.array(indices) - 1\n",
    "                        # create np array of the same size as the signal and fill it with zeros (default value), put 1 in the indices\n",
    "                        # of the beats\n",
    "                        beats = np.zeros(signals.shape[0])\n",
    "                        beats[indices] = 1\n",
    "\n",
    "                        # create np array with dims [2, len(signals)] to store the signal and the beats\n",
    "                        data = np.vstack((signals[:, 0], beats))\n",
    "                    else:\n",
    "                        data = signals[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "                    # Save the p_signal data in the HDF5 file with the dataset name\n",
    "                    h5_file.create_dataset(dataset_name, data=data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_dir = os.path.join(data_path, 'icentia11k-continuous-ecg_normal_sinus_subset')\n",
    "pSignal_npArray_data_dir_h5 = os.path.join(data_path,\n",
    "                                           \"with_R_beats\",\n",
    "                                           'icentia11k-continuous-ecg_normal_sinus_subset_npArrays.h5'\n",
    "                                           )\n",
    "# data_preparation.extract_and_save_p_signal_to_HDF5(subset_data_dir, pSignal_npArray_data_dir_h5, with_R_beats=True)\n",
    "extract_and_save_p_signal_to_HDF5(subset_data_dir, pSignal_npArray_data_dir_h5, with_R_beats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bfe1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_first_n_datasets_in_HDF5(hdf5_file, n=10):\n",
    "    \"\"\"\n",
    "    Print the first n datasets in an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    - hdf5_file: The HDF5 file to read the data from.\n",
    "    - n: The number of datasets to print.\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_file, 'r') as h5_file:\n",
    "        datasets = []\n",
    "\n",
    "        def visitor_func(name, node):\n",
    "            if isinstance(node, h5py.Dataset):\n",
    "                datasets.append(name)\n",
    "\n",
    "        h5_file.visititems(visitor_func)\n",
    "        for dataset in datasets[:n]:\n",
    "            print(dataset)\n",
    "\n",
    "# Print the first 10 datasets in the HDF5 file\n",
    "# print_first_n_datasets_in_HDF5(pSignal_npArray_data_dir_h5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7581480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_content(hdf5_file, dataset_name):\n",
    "    \"\"\"\n",
    "    Read the content of a specific dataset from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    - hdf5_file: The HDF5 file to read the data from.\n",
    "    - dataset_name: The name of the dataset to read.\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_file, 'r') as h5_file:\n",
    "        if dataset_name in h5_file:\n",
    "            data = h5_file[dataset_name][:]\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Dataset {dataset_name} not found in the file.\")\n",
    "            return None\n",
    "\n",
    "# Read the content of a specific dataset\n",
    "data = read_dataset_content(pSignal_npArray_data_dir_h5, \"p00/p00000/p00000_s00_128028_to_281500_p_signal\")\n",
    "print(f\"{data.shape=}\")\n",
    "print(f\"{data=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636626a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation.count_items(pSignal_npArray_data_dir_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation.print_h5_hierarchy(pSignal_npArray_data_dir_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def split_and_save_data(input_h5_file, window_size, output_h5_file):\n",
    "    \"\"\"\n",
    "    Split each dataset in the input HDF5 file into windows of the specified size\n",
    "    and save the resulting windows into an output HDF5 file.\n",
    "\n",
    "    :param input_h5_file: The input HDF5 file with datasets to split.\n",
    "    :param window_size: The size of each window.\n",
    "    :param output_h5_file: The output HDF5 file to save the split data.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    def extract_integers(text):\n",
    "        \"\"\"\n",
    "        Extract integers from the given text.\n",
    "\n",
    "        :param text: The input text containing characters and integers.\n",
    "        :return: A string containing only the integers found in the text.\n",
    "        \"\"\"\n",
    "        return ''.join(filter(str.isdigit, str(text)))\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_h5_file), exist_ok=True)\n",
    "\n",
    "    #check if file is being written by another process\n",
    "    import fcntl\n",
    "\n",
    "    def is_file_locked(file_path):\n",
    "        locked = None\n",
    "        file_object = open(file_path, 'r')\n",
    "        try:\n",
    "            fcntl.flock(file_object, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "            locked = False\n",
    "        except IOError:\n",
    "            locked = True\n",
    "        finally:\n",
    "            file_object.close()\n",
    "        return locked\n",
    "\n",
    "    # Usage\n",
    "    if is_file_locked(input_h5_file):\n",
    "        print(f\"The file {input_h5_file} is being written by another process.\")\n",
    "    else:\n",
    "        print(f\"The file {input_h5_file} is not locked.\")\n",
    "\n",
    "    print(f\"{os.path.exists(input_h5_file)=}, {os.path.exists(output_h5_file)=}\")\n",
    "    with h5py.File(input_h5_file, 'r') as input_file, h5py.File(output_h5_file, 'w') as output_file:\n",
    "        total_leaf_iterations = 0\n",
    "        for group_name, group_item in input_file.items():\n",
    "            assert not isinstance(group_name, h5py.Group), \"create only leaf groups\"\n",
    "            for subgroup_name, subgroup_item in tqdm(group_item.items(), desc=\"Processing Subgroups\", unit=\"subgroup\"):\n",
    "                # print(f\"subgroup_name: {subgroup_name}\")\n",
    "                assert not isinstance(subgroup_name, h5py.Group), \"leaf groups\"\n",
    "                # dataset_data = []\n",
    "                total_leaf_iterations += len(subgroup_item)\n",
    "\n",
    "        progress_bar = tqdm(total=total_leaf_iterations, position=0, desc='Processing')\n",
    "        for group_name, group_item in input_file.items():\n",
    "            assert not isinstance(group_name, h5py.Group), \"create only leaf groups\"\n",
    "            for subgroup_name, subgroup_item in group_item.items():\n",
    "                # print(f\"subgroup_name: {subgroup_name}\")\n",
    "                assert not isinstance(subgroup_name, h5py.Group), \"leaf groups\"\n",
    "                dataset_data = []\n",
    "                for dataset_name, dataset_item in subgroup_item.items():\n",
    "                    # print(f\"dataset_name: {dataset_name}\")\n",
    "                    assert not isinstance(dataset_name, h5py.Dataset)\n",
    "                    # Split the dataset into windows\n",
    "                    data = dataset_item[:] # data.shape = (2, len(signal))\n",
    "                    num_windows = len(data[1]) // window_size\n",
    "                    if num_windows > 1:\n",
    "                        pass\n",
    "                    \n",
    "                    # window_data = np.array([data[:, i:i + window_size] for i in range(0, data.shape[1], window_size)])\n",
    "                    window_data = np.array([data[:, i:i + window_size] for i in range(0, data.shape[1], window_size) if i + window_size <= data.shape[1]])\n",
    "                    dataset_data.extend(window_data)\n",
    "                    # Save each window as numpy array and add it to the output dataset\n",
    "                    # for i in range(num_windows):\n",
    "                    #     window_data = data[i * window_size: (i + 1) * window_size]\n",
    "                        \n",
    "                    #     dataset_data.append(window_data)\n",
    "\n",
    "                    dataset_name = extract_integers(subgroup_name)\n",
    "                    progress_bar.update(1)\n",
    "                output_file.create_dataset(dataset_name, data=dataset_data)\n",
    "                \n",
    "# split the arrays to fixed size windows\n",
    "fs = 250\n",
    "context_window_size = 9*60*fs  # minutes * seconds * fs\n",
    "label_window_size = 1*60*fs  # minutes * seconds * fs\n",
    "window_size = context_window_size+label_window_size\n",
    "\n",
    "split_pSignal_file = os.path.join(data_path,\n",
    "                                    \"with_R_beats\",\n",
    "                                    'icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits',\n",
    "                                    '10minutes_window.h5')\n",
    "base_name, extension = os.path.splitext(os.path.basename(split_pSignal_file))\n",
    "new_base_name = f\"{base_name}_temp{extension}\"\n",
    "temp_filename = os.path.join(os.path.dirname(split_pSignal_file), new_base_name)\n",
    "# data_preparation.split_and_save_data(pSignal_npArray_data_dir_h5, window_size, split_pSignal_file)\n",
    "split_and_save_data(pSignal_npArray_data_dir_h5, window_size, split_pSignal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n_datasets_in_HDF5(split_pSignal_file, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dataset_content(split_pSignal_file, \"00000\")\n",
    "print(f\"{data.shape=}\")\n",
    "print(f\"{data[0].shape=}\")\n",
    "print(f\"{data[0][1]=}\")\n",
    "indices = np.where(data[0][1] == 1)\n",
    "print(f\"{len(indices)=}\")\n",
    "print(f\"{indices[0].shape=}\")\n",
    "print(f\"Indices of 1: {indices[0][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a5b726bd64e77",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T13:40:16.638542400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_preparation.print_h5_hierarchy(split_pSignal_file)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966465396a9676df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T13:50:58.089907600Z",
     "start_time": "2024-02-14T13:50:57.956323100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def print_top_items(file_path, num_datasets=3, num_items=3):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        dataset_counter = 0\n",
    "        for name, item in f.items():\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                print(f\"Dataset: {name}\")\n",
    "                print(f\"top {num_items} items:\")\n",
    "                print(item[:num_items])\n",
    "                dataset_counter += 1\n",
    "                if dataset_counter >= num_datasets:\n",
    "                    break\n",
    "\n",
    "print_top_items(split_pSignal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d7848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 250 #sampling rate\n",
    "SECONDS_IN_MINUTE = 60\n",
    "# split the arrays to fixed size windows\n",
    "context_window_size = 10*SECONDS_IN_MINUTE*fs  # minutes * seconds * fs\n",
    "label_window_size = 5*SECONDS_IN_MINUTE*fs  # minutes * seconds * fs\n",
    "window_size = context_window_size+label_window_size\n",
    "\n",
    "# creating subset of normal sinus rhythms (NSR) from the raw data\n",
    "min_window_size = window_size  # minutes * seconds * sampling rate\n",
    "# the reason for min_window_size is that I hope to forecast 1-5 minutes ahead.\n",
    "# and I dont know if a smaller window will give me enough context data\n",
    "# on top of that, I think I have enough data so I can fiter out the shorter NSR.\n",
    "\n",
    "data_dir = '/datasets/Icentia11k/physionet.org/files/icentia11k-continuous-ecg/1.0'\n",
    "my_data_dir = '/home/liranc6/data'\n",
    "\n",
    "# after creating the subset, with 10 first patients I have more than 10 hours of NSR data\n",
    "# divided to 62 files of at least 10 minutes each.\n",
    "# I know its small but I dont need more for now. when I will, I will add more patients. (I used 10/11000 patients)\n",
    "# subset_data_dir = os.path.join(my_data_dir, 'icentia11k-continuous-ecg_normal_sinus_subset')\n",
    "\n",
    "pSignal_npArray_data_dir_h5 = os.path.join(my_data_dir, \"with_R_beats\", 'icentia11k-continuous-ecg_normal_sinus_subset_npArrays.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb00ff6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays.h5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pSignal_npArray_data_dir_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3528a1ca2e08566",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract_sinus_rhythms_to_new_subset: 100%|██████████| 400/400 [00:25<00:00, 15.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import h5py\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "parallel_extract_sinus_rhythms_to_p_signal_array(data_dir, pSignal_npArray_data_dir_h5, min_window_size, start_patient_id=47, end_patient_id=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d649a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /00000, Count: 90\n",
      "Dataset: /00001, Count: 50\n",
      "Dataset: /00002, Count: 32\n",
      "Dataset: /00003, Count: 14\n",
      "Dataset: /00004, Count: 15\n",
      "Dataset: /00005, Count: 43\n",
      "Dataset: /00006, Count: 35\n",
      "Dataset: /00007, Count: 91\n",
      "Dataset: /00008, Count: 41\n",
      "Dataset: /00009, Count: 13\n",
      "Dataset: /00011, Count: 65\n",
      "Dataset: /00012, Count: 10\n",
      "Dataset: /00013, Count: 126\n",
      "Dataset: /00014, Count: 155\n",
      "Dataset: /00015, Count: 75\n",
      "Dataset: /00016, Count: 82\n",
      "Dataset: /00017, Count: 13\n",
      "Dataset: /00018, Count: 159\n",
      "Dataset: /00019, Count: 75\n",
      "Dataset: /00020, Count: 20\n",
      "Dataset: /00021, Count: 30\n",
      "Dataset: /00022, Count: 81\n",
      "Dataset: /00023, Count: 35\n",
      "Dataset: /00024, Count: 52\n",
      "Dataset: /00025, Count: 38\n",
      "Dataset: /00026, Count: 16\n",
      "Dataset: /00027, Count: 72\n",
      "Dataset: /00028, Count: 69\n",
      "Dataset: /00029, Count: 104\n",
      "Dataset: /00030, Count: 32\n",
      "Dataset: /00031, Count: 31\n",
      "Dataset: /00032, Count: 34\n",
      "Dataset: /00033, Count: 29\n",
      "Dataset: /00034, Count: 64\n",
      "Dataset: /00035, Count: 97\n",
      "Dataset: /00036, Count: 67\n",
      "Dataset: /00038, Count: 45\n",
      "Dataset: /00039, Count: 98\n",
      "Dataset: /00040, Count: 17\n",
      "Dataset: /00041, Count: 44\n",
      "Dataset: /00042, Count: 56\n",
      "Dataset: /00043, Count: 83\n",
      "Dataset: /00044, Count: 76\n",
      "Dataset: /00045, Count: 84\n",
      "Dataset: /00046, Count: 8\n",
      "Dataset: /00047, Count: 43\n",
      "Dataset: /00048, Count: 80\n",
      "Dataset: /00049, Count: 59\n",
      "Dataset: /00050, Count: 57\n",
      "Dataset: /00051, Count: 2\n",
      "Dataset: /00052, Count: 50\n",
      "Dataset: /00053, Count: 62\n",
      "Dataset: /00054, Count: 145\n",
      "Dataset: /00055, Count: 56\n",
      "Dataset: /00056, Count: 7\n",
      "Dataset: /00057, Count: 63\n",
      "Dataset: /00058, Count: 52\n",
      "Dataset: /00059, Count: 59\n",
      "Dataset: /00061, Count: 46\n",
      "Dataset: /00062, Count: 49\n",
      "Dataset: /00063, Count: 64\n",
      "Dataset: /00064, Count: 56\n",
      "Dataset: /00066, Count: 25\n",
      "Dataset: /00067, Count: 43\n",
      "Dataset: /00068, Count: 13\n",
      "Dataset: /00069, Count: 24\n",
      "Dataset: /00070, Count: 52\n",
      "Dataset: /00071, Count: 61\n",
      "Dataset: /00072, Count: 78\n",
      "Dataset: /00073, Count: 42\n",
      "Dataset: /00074, Count: 33\n",
      "Dataset: /00076, Count: 14\n",
      "Dataset: /00077, Count: 51\n",
      "Dataset: /00078, Count: 29\n",
      "Dataset: /00079, Count: 1\n",
      "Dataset: /00080, Count: 25\n",
      "Dataset: /00081, Count: 47\n",
      "Dataset: /00083, Count: 67\n",
      "Dataset: /00084, Count: 85\n",
      "Dataset: /00085, Count: 92\n",
      "Dataset: /00086, Count: 27\n",
      "Dataset: /00087, Count: 26\n",
      "Dataset: /00088, Count: 135\n",
      "Dataset: /00089, Count: 19\n",
      "Dataset: /00090, Count: 26\n",
      "Dataset: /00092, Count: 62\n",
      "Dataset: /00093, Count: 20\n",
      "Dataset: /00094, Count: 32\n",
      "Dataset: /00095, Count: 95\n",
      "Dataset: /00096, Count: 41\n",
      "Dataset: /00097, Count: 29\n",
      "Dataset: /00098, Count: 60\n",
      "Dataset: /00099, Count: 17\n",
      "Dataset: /00100, Count: 24\n",
      "Dataset: /00101, Count: 51\n",
      "Dataset: /00102, Count: 75\n",
      "Dataset: /00103, Count: 36\n",
      "Dataset: /00104, Count: 42\n",
      "Dataset: /00105, Count: 69\n",
      "Dataset: /00106, Count: 15\n",
      "Dataset: /00107, Count: 64\n",
      "Dataset: /00108, Count: 10\n",
      "Dataset: /00109, Count: 36\n",
      "Dataset: /00110, Count: 22\n",
      "Dataset: /00111, Count: 46\n",
      "Dataset: /00112, Count: 15\n",
      "Dataset: /00113, Count: 47\n",
      "Dataset: /00115, Count: 31\n",
      "Dataset: /00116, Count: 69\n",
      "Dataset: /00117, Count: 44\n",
      "Dataset: /00118, Count: 29\n",
      "Dataset: /00119, Count: 60\n",
      "Dataset: /00120, Count: 31\n",
      "Dataset: /00121, Count: 34\n",
      "Dataset: /00122, Count: 67\n",
      "Dataset: /00123, Count: 39\n",
      "Dataset: /00124, Count: 33\n",
      "Dataset: /00125, Count: 20\n",
      "Dataset: /00126, Count: 63\n",
      "Dataset: /00127, Count: 14\n",
      "Dataset: /00128, Count: 36\n",
      "Dataset: /00129, Count: 136\n",
      "Dataset: /00130, Count: 115\n",
      "Dataset: /00131, Count: 35\n",
      "Dataset: /00132, Count: 71\n",
      "Dataset: /00133, Count: 17\n",
      "Dataset: /00134, Count: 88\n",
      "Dataset: /00136, Count: 72\n",
      "Dataset: /00137, Count: 27\n",
      "Dataset: /00138, Count: 52\n",
      "Dataset: /00139, Count: 47\n",
      "Dataset: /00140, Count: 45\n",
      "Dataset: /00141, Count: 158\n",
      "Dataset: /00142, Count: 111\n",
      "Dataset: /00143, Count: 54\n",
      "Dataset: /00144, Count: 32\n",
      "Dataset: /00145, Count: 14\n",
      "Dataset: /00146, Count: 69\n",
      "Dataset: /00147, Count: 88\n",
      "Dataset: /00148, Count: 79\n",
      "Dataset: /00149, Count: 29\n",
      "Dataset: /00150, Count: 41\n",
      "Dataset: /00151, Count: 68\n",
      "Dataset: /00152, Count: 99\n",
      "Dataset: /00153, Count: 45\n",
      "Dataset: /00154, Count: 32\n",
      "Dataset: /00155, Count: 5\n",
      "Dataset: /00156, Count: 113\n",
      "Dataset: /00157, Count: 47\n",
      "Dataset: /00158, Count: 79\n",
      "Dataset: /00160, Count: 43\n",
      "Dataset: /00161, Count: 31\n",
      "Dataset: /00162, Count: 82\n",
      "Dataset: /00163, Count: 12\n",
      "Dataset: /00164, Count: 39\n",
      "Dataset: /00165, Count: 38\n",
      "Dataset: /00167, Count: 35\n",
      "Dataset: /00168, Count: 11\n",
      "Dataset: /00169, Count: 39\n",
      "Dataset: /00170, Count: 41\n",
      "Dataset: /00172, Count: 48\n",
      "Dataset: /00173, Count: 96\n",
      "Dataset: /00174, Count: 69\n",
      "Dataset: /00175, Count: 25\n",
      "Dataset: /00176, Count: 28\n",
      "Dataset: /00178, Count: 88\n",
      "Dataset: /00179, Count: 174\n",
      "Dataset: /00180, Count: 42\n",
      "Dataset: /00181, Count: 44\n",
      "Dataset: /00182, Count: 56\n",
      "Dataset: /00183, Count: 65\n",
      "Dataset: /00184, Count: 31\n",
      "Dataset: /00185, Count: 50\n",
      "Dataset: /00186, Count: 55\n",
      "Dataset: /00187, Count: 34\n",
      "Dataset: /00188, Count: 42\n",
      "Dataset: /00189, Count: 38\n",
      "Dataset: /00190, Count: 71\n",
      "Dataset: /00191, Count: 8\n",
      "Dataset: /00192, Count: 69\n",
      "Dataset: /00193, Count: 87\n",
      "Dataset: /00194, Count: 41\n",
      "Dataset: /00195, Count: 63\n",
      "Dataset: /00196, Count: 40\n",
      "Dataset: /00197, Count: 23\n",
      "Dataset: /00198, Count: 36\n",
      "Dataset: /00199, Count: 74\n",
      "Dataset: /00200, Count: 47\n",
      "Dataset: /00201, Count: 74\n",
      "Dataset: /00203, Count: 35\n",
      "Dataset: /00204, Count: 12\n",
      "Dataset: /00205, Count: 23\n",
      "Dataset: /00206, Count: 17\n",
      "Dataset: /00207, Count: 143\n",
      "Dataset: /00208, Count: 12\n",
      "Dataset: /00209, Count: 9\n",
      "Dataset: /00210, Count: 34\n",
      "Dataset: /00211, Count: 98\n",
      "Dataset: /00212, Count: 32\n",
      "Dataset: /00213, Count: 24\n",
      "Dataset: /00214, Count: 19\n",
      "Dataset: /00215, Count: 31\n",
      "Dataset: /00216, Count: 61\n",
      "Dataset: /00217, Count: 79\n",
      "Dataset: /00218, Count: 44\n",
      "Dataset: /00219, Count: 39\n",
      "Dataset: /00220, Count: 68\n",
      "Dataset: /00221, Count: 30\n",
      "Dataset: /00222, Count: 68\n",
      "Dataset: /00223, Count: 51\n",
      "Dataset: /00224, Count: 92\n",
      "Dataset: /00225, Count: 51\n",
      "Dataset: /00226, Count: 25\n",
      "Dataset: /00227, Count: 107\n",
      "Dataset: /00228, Count: 82\n",
      "Dataset: /00229, Count: 9\n",
      "Dataset: /00231, Count: 33\n",
      "Dataset: /00232, Count: 19\n",
      "Dataset: /00233, Count: 68\n",
      "Dataset: /00234, Count: 39\n",
      "Dataset: /00235, Count: 46\n",
      "Dataset: /00236, Count: 51\n",
      "Dataset: /00237, Count: 17\n",
      "Dataset: /00238, Count: 133\n",
      "Dataset: /00239, Count: 16\n",
      "Dataset: /00240, Count: 27\n",
      "Dataset: /00241, Count: 65\n",
      "Dataset: /00242, Count: 44\n",
      "Dataset: /00243, Count: 12\n",
      "Dataset: /00244, Count: 40\n",
      "Dataset: /00245, Count: 92\n",
      "Dataset: /00246, Count: 41\n",
      "Dataset: /00248, Count: 49\n",
      "Dataset: /00249, Count: 36\n",
      "Dataset: /00250, Count: 25\n",
      "Dataset: /00251, Count: 98\n",
      "Dataset: /00252, Count: 51\n",
      "Dataset: /00253, Count: 18\n",
      "Dataset: /00254, Count: 42\n",
      "Dataset: /00255, Count: 72\n",
      "Dataset: /00256, Count: 81\n",
      "Dataset: /00257, Count: 68\n",
      "Dataset: /00258, Count: 62\n",
      "Dataset: /00259, Count: 51\n",
      "Dataset: /00260, Count: 28\n",
      "Dataset: /00261, Count: 56\n",
      "Dataset: /00262, Count: 58\n",
      "Dataset: /00263, Count: 4\n",
      "Dataset: /00264, Count: 37\n",
      "Dataset: /00265, Count: 47\n",
      "Dataset: /00266, Count: 182\n",
      "Dataset: /00267, Count: 29\n",
      "Dataset: /00269, Count: 19\n",
      "Dataset: /00270, Count: 17\n",
      "Dataset: /00272, Count: 89\n",
      "Dataset: /00273, Count: 30\n",
      "Dataset: /00274, Count: 38\n",
      "Dataset: /00275, Count: 64\n",
      "Dataset: /00276, Count: 50\n",
      "Dataset: /00277, Count: 42\n",
      "Dataset: /00278, Count: 27\n",
      "Dataset: /00279, Count: 66\n",
      "Dataset: /00280, Count: 42\n",
      "Dataset: /00281, Count: 53\n",
      "Dataset: /00282, Count: 51\n",
      "Dataset: /00283, Count: 41\n",
      "Dataset: /00284, Count: 113\n",
      "Dataset: /00285, Count: 72\n",
      "Dataset: /00286, Count: 69\n",
      "Dataset: /00288, Count: 81\n",
      "Dataset: /00289, Count: 53\n",
      "Dataset: /00290, Count: 66\n",
      "Dataset: /00291, Count: 16\n",
      "Dataset: /00293, Count: 39\n",
      "Dataset: /00294, Count: 76\n",
      "Dataset: /00295, Count: 7\n",
      "Dataset: /00296, Count: 37\n",
      "Dataset: /00297, Count: 116\n",
      "Dataset: /00298, Count: 43\n",
      "Dataset: /00300, Count: 49\n",
      "Dataset: /00301, Count: 54\n",
      "Dataset: /00302, Count: 52\n",
      "Dataset: /00304, Count: 119\n",
      "Dataset: /00305, Count: 13\n",
      "Dataset: /00306, Count: 67\n",
      "Dataset: /00307, Count: 34\n",
      "Dataset: /00308, Count: 31\n",
      "Dataset: /00309, Count: 36\n",
      "Dataset: /00310, Count: 67\n",
      "Dataset: /00311, Count: 33\n",
      "Dataset: /00312, Count: 54\n",
      "Dataset: /00313, Count: 60\n",
      "Dataset: /00314, Count: 93\n",
      "Dataset: /00315, Count: 25\n",
      "Dataset: /00316, Count: 14\n",
      "Dataset: /00317, Count: 26\n",
      "Dataset: /00318, Count: 26\n",
      "Dataset: /00319, Count: 60\n",
      "Dataset: /00320, Count: 44\n",
      "Dataset: /00321, Count: 37\n",
      "Dataset: /00322, Count: 83\n",
      "Dataset: /00324, Count: 51\n",
      "Dataset: /00325, Count: 32\n",
      "Dataset: /00326, Count: 148\n",
      "Dataset: /00327, Count: 42\n",
      "Dataset: /00329, Count: 23\n",
      "Dataset: /00330, Count: 19\n",
      "Dataset: /00331, Count: 67\n",
      "Dataset: /00332, Count: 6\n",
      "Dataset: /00333, Count: 32\n",
      "Dataset: /00334, Count: 84\n",
      "Dataset: /00335, Count: 34\n",
      "Dataset: /00336, Count: 29\n",
      "Dataset: /00338, Count: 29\n",
      "Dataset: /00339, Count: 35\n",
      "Dataset: /00340, Count: 18\n",
      "Dataset: /00341, Count: 2\n",
      "Dataset: /00342, Count: 28\n",
      "Dataset: /00343, Count: 51\n",
      "Dataset: /00344, Count: 34\n",
      "Dataset: /00345, Count: 96\n",
      "Dataset: /00346, Count: 88\n",
      "Dataset: /00347, Count: 60\n",
      "Dataset: /00348, Count: 51\n",
      "Dataset: /00349, Count: 36\n",
      "Dataset: /00350, Count: 49\n",
      "Dataset: /00351, Count: 59\n",
      "Dataset: /00352, Count: 33\n",
      "Dataset: /00353, Count: 127\n",
      "Dataset: /00354, Count: 110\n",
      "Dataset: /00355, Count: 36\n",
      "Dataset: /00356, Count: 53\n",
      "Dataset: /00357, Count: 33\n",
      "Dataset: /00358, Count: 79\n",
      "Dataset: /00359, Count: 14\n",
      "Dataset: /00360, Count: 29\n",
      "Dataset: /00361, Count: 30\n",
      "Dataset: /00362, Count: 102\n",
      "Dataset: /00363, Count: 43\n",
      "Dataset: /00364, Count: 66\n",
      "Dataset: /00366, Count: 69\n",
      "Dataset: /00367, Count: 43\n",
      "Dataset: /00368, Count: 20\n",
      "Dataset: /00369, Count: 60\n",
      "Dataset: /00370, Count: 52\n",
      "Dataset: /00371, Count: 46\n",
      "Dataset: /00372, Count: 155\n",
      "Dataset: /00373, Count: 62\n",
      "Dataset: /00374, Count: 68\n",
      "Dataset: /00375, Count: 21\n",
      "Dataset: /00376, Count: 98\n",
      "Dataset: /00377, Count: 32\n",
      "Dataset: /00378, Count: 45\n",
      "Dataset: /00379, Count: 32\n",
      "Dataset: /00380, Count: 58\n",
      "Dataset: /00381, Count: 40\n",
      "Dataset: /00382, Count: 90\n",
      "Dataset: /00383, Count: 55\n",
      "Dataset: /00384, Count: 22\n",
      "Dataset: /00385, Count: 33\n",
      "Dataset: /00386, Count: 30\n",
      "Dataset: /00387, Count: 35\n",
      "Dataset: /00388, Count: 72\n",
      "Dataset: /00389, Count: 40\n",
      "Dataset: /00390, Count: 22\n",
      "Dataset: /00391, Count: 69\n",
      "Dataset: /00392, Count: 29\n",
      "Dataset: /00393, Count: 18\n",
      "Dataset: /00394, Count: 68\n",
      "Dataset: /00395, Count: 66\n",
      "Dataset: /00396, Count: 20\n",
      "Dataset: /00397, Count: 69\n",
      "Dataset: /00398, Count: 45\n",
      "Dataset: /00399, Count: 80\n",
      "Total count: 19214\n"
     ]
    }
   ],
   "source": [
    "path = '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/15_minutes/train/p0_to_p399.h5'\n",
    "\n",
    "data_preparation.count_items(path)\n",
    "# data_preparation.read_dataset_content(path, '00000').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
