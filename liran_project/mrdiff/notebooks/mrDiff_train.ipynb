{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import yaml\n",
    "from box import Box\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "CONFIG_FILENAME = '/home/liranc6/ecg_forecasting/liran_project/mrdiff/src/config_ecg.yml'\n",
    "\n",
    "assert CONFIG_FILENAME.endswith('.yml')\n",
    "\n",
    "with open(CONFIG_FILENAME, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "ProjectPath = config['project_path']\n",
    "sys.path.append(ProjectPath)\n",
    "\n",
    "from liran_project.mrdiff.src.parser import parse_args\n",
    "from liran_project.utils.dataset_loader import SingleLeadECGDatasetCrops_mrDiff as DataSet\n",
    "from liran_project.utils.util import ecg_signal_difference\n",
    "# from liran_project.mrdiff.exp_main import Exp_Main\n",
    "from liran_project.utils.common import *\n",
    "\n",
    "# Add the directory containing the exp module to the sys.path\n",
    "exp_module_path = os.path.join(ProjectPath, 'mrDiff')\n",
    "sys.path.append(exp_module_path)\n",
    "\n",
    "# from mrDiff.exp.exp_main import Exp_Main\n",
    "from mrDiff.data_process.etth_dataloader import Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom, Dataset_Wind, Dataset_Caiso, Dataset_Production, Dataset_Caiso_M, Dataset_Production_M\n",
    "from mrDiff.data_process.financial_dataloader import DatasetH\n",
    "from mrDiff.data_process.forecast_dataloader import ForecastDataset\n",
    "from mrDiff.exp.exp_basic import Exp_Basic\n",
    "from mrDiff.models_diffusion import DDPM\n",
    "from mrDiff.utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from mrDiff.utils.metrics import metric\n",
    "\n",
    "from liran_project.mrdiff.src.parser import Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(CONFIG_FILENAME)\n",
    "\n",
    "# Now you can use args as needed\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Box object to dictionary\n",
    "config_dict = args.configs.to_dict()\n",
    "\n",
    "# Access the configuration values using dictionary syntax\n",
    "random_seed = config_dict['general']['random_seed']\n",
    "tag = config_dict['general']['tag']\n",
    "dataset = config_dict['general']['dataset']\n",
    "features = config_dict['general']['features']\n",
    "\n",
    "learning_rate = config_dict['optimization']['learning_rate']\n",
    "batch_size = config_dict['optimization']['batch_size']\n",
    "\n",
    "context_len = config_dict['training']['sequence']['context_len']\n",
    "label_len = config_dict['training']['sequence']['label_len']\n",
    "model = config_dict['training']['model_info']['model']\n",
    "pred_len = config_dict['training']['sequence']['pred_len']\n",
    "iterations = config_dict['training']['iterations']['itr']\n",
    "\n",
    "inverse = config_dict['data']['inverse']\n",
    "    \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False\n",
    "torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb\n",
    "wandb_init_config ={\n",
    "        \"entity\": args.wandb.entity,\n",
    "        \"mode\": args.wandb.mode,\n",
    "        \"project\": args.wandb.project,\n",
    "        \"save_code\": args.wandb.save_code,\n",
    "    }\n",
    "wandb_init_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project_name = args.wandb.project\n",
    "wandb_id = args.wandb.id if args.wandb.id != \"None\" else None\n",
    "wandb_mode = args.wandb.mode if args.wandb.mode != \"None\" else \"online\"\n",
    "wandb_resume = args.wandb.resume if args.wandb.resume != \"None\" else None\n",
    "wandb.init(project=wandb_project_name, id=wandb_id, resume=wandb_resume, mode=wandb_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.wandb.resume != \"None\":\n",
    "    wandb_init_config.update({\n",
    "                            \"id\": args.wandb.id,\n",
    "                            \"resume\": args.wandb.resume\n",
    "                            })\n",
    "    \n",
    "    if args.wandb.resume_from != \"None\":\n",
    "        wandb_init_config[\"config\"] = args.wandb.resume_from\n",
    "        \n",
    "    run = wandb.init(**wandb_init_config)\n",
    "    print(f\"Resuming wandb run id: {wandb.run.id}\")\n",
    "    \n",
    "    def log_config_diffs(old_config, new_config, step):\n",
    "        diffs = {}\n",
    "        for key in new_config:\n",
    "            if key not in old_config or old_config[key] != new_config[key]:\n",
    "                diffs[key] = {'old': old_config.get(key), 'new': new_config[key]}\n",
    "    \n",
    "        if diffs:\n",
    "            note = f\"Config changes at step {step}:\\n\"\n",
    "            for key, value in diffs.items():\n",
    "                note += f\"{key}: {value['old']} -> {value['new']}\\n\"\n",
    "            wandb.run.notes = (wandb.run.notes or \"\") + note + \"\\n\\nAdditional information added later:\\n\"\n",
    "    \n",
    "    old_config = wandb.config.copy()\n",
    "    wandb.config.update(args)\n",
    "    new_config = wandb.config.copy()\n",
    "    log_config_diffs(old_config, new_config, step=\"update_args\")           \n",
    "else:\n",
    "    wandb.init(**wandb_init_config, config=args, )\n",
    "    print(f\"New wandb run id: {wandb.run.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "iteration = 1\n",
    "# setting\n",
    "setting = f\"{model}_{dataset}_ft{features}_sl{context_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iteration}\"\n",
    "\n",
    "if tag is not None:\n",
    "    setting += f\"_{tag}\"\n",
    "\n",
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = liran_project.mrdiff.exp_main.Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.args.configs_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.read_data('train')\n",
    "exp.read_data('val')\n",
    "exp.read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module\n",
    "importlib.reload(liran_project.mrdiff.exp_main)\n",
    "\n",
    "# Assuming `exp` is an existing instance of `Exp_Main`\n",
    "exp.__class__ = liran_project.mrdiff.exp_main.Exp_Main\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "! gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.print_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "try:\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            # torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        exp.train(setting)\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during training: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming `prof` is your profiler object\n",
    "key_averages = prof.key_averages()\n",
    "\n",
    "# Filter to include only CUDA operations\n",
    "cuda_operations = [item for item in key_averages if 'cuda' in item.key]\n",
    "\n",
    "# Print the filtered table\n",
    "print(key_averages.table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# Assuming `prof` is your profiler object\n",
    "key_averages = prof.key_averages()\n",
    "\n",
    "# Total CUDA memory usage\n",
    "total_cuda_memory_usage = sum(item.self_cuda_memory_usage for item in key_averages)\n",
    "print(f\"Total CUDA Memory Usage: {total_cuda_memory_usage} bytes\")\n",
    "\n",
    "# Memory usage by function\n",
    "print(key_averages.table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp.model_start_training_time = \"02_10_2024_135344\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "exp.test(setting, test=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(iterations):\n",
    "    # setting record of experiments\n",
    "\n",
    "    # random seed\n",
    "    \n",
    "    # setting\n",
    "    setting = f\"{model}_{dataset}_ft{features}_sl{context_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iteration}\"\n",
    "    \n",
    "    if tag is not None:\n",
    "        setting += f\"_{tag}\"\n",
    "\n",
    "    exp = Exp_Main(args)\n",
    "\n",
    "    print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    exp.train(setting)\n",
    "\n",
    "    print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    exp.test(setting, test=1)\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.configs.general.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_samples = 0\n",
    "\n",
    "with h5py.File(filename, 'r') as h5_file:\n",
    "    num_keys = len(self.keys)\n",
    "    pbar_keys = tqdm(self.keys, total=num_keys, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pbar_keys.set_description(f\"creating_stats\")\n",
    "    for key in pbar_keys:\n",
    "        data = h5_file[key][()][:, 0, :] if self.data_with_RR else h5_file[key][()]\n",
    "            \n",
    "        curr_num_samples = data.shape[0]\n",
    "        total_num_samples += curr_num_samples\n",
    "\n",
    "        max_val = max(max_val, np.max(data))\n",
    "        min_val = min(min_val, np.min(data))\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Update the postfix\n",
    "        pbar_keys.set_postfix({\"time_elapsed\": str(timedelta(seconds=int(elapsed_time)))})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                    label_len, pred_len, learning_rate, batch_size, inverse, tag, args):\n",
    "    mae_ = []\n",
    "    mse_ = []\n",
    "    rmse_ = []\n",
    "    mape_ = []\n",
    "    mspe_ = []\n",
    "    rse_ = []\n",
    "    corr_ = []\n",
    "    nrmse_ = []\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        # setting record of experiments\n",
    "\n",
    "        # random seed\n",
    "        fix_seed = iter if iterations > 1 else random_seed\n",
    "\n",
    "        random.seed(fix_seed)\n",
    "        torch.manual_seed(fix_seed)\n",
    "        np.random.seed(fix_seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        setting = f\"{model}_{dataset}_ft{features}_sl{seq_len}_ll{label_len}_pl{pred_len}_lr{learning_rate}_bs{batch_size}_inv{inverse}_itr{iter}\"\n",
    "        if tag is not None:\n",
    "            setting += f\"_{tag}\"\n",
    "\n",
    "        exp = Exp_Main(args)\n",
    "\n",
    "        print(f'>>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "        exp.train(setting)\n",
    "\n",
    "        print(f'>>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "        mae, mse, rmse, mape, mspe, rse, corr, nrmse = exp.test(setting, test=1)\n",
    "\n",
    "        mae_.append(mae)\n",
    "        mse_.append(mse)\n",
    "        rmse_.append(rmse)\n",
    "        mape_.append(mape)\n",
    "        mspe_.append(mspe)\n",
    "        rse_.append(rse)\n",
    "        corr_.append(corr)\n",
    "        nrmse_.append(nrmse)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print('Final mean normed: ')\n",
    "    print('> mae:{:.4f}, std:{:.4f}'.format(np.mean(mae_), np.std(mae_)))\n",
    "    print('> mse:{:.4f}, std:{:.4f}'.format(np.mean(mse_), np.std(mse_)))\n",
    "    print('> rmse:{:.4f}, std:{:.4f}'.format(np.mean(rmse_), np.std(rmse_)))\n",
    "    print('> mape:{:.4f}, std:{:.4f}'.format(np.mean(mape_), np.std(mape_)))\n",
    "    print('> rse:{:.4f}, std:{:.4f}'.format(np.mean(rse_), np.std(rse_)))\n",
    "    print('> corr:{:.4f}, std:{:.4f}'.format(np.mean(corr_), np.std(corr_)))\n",
    "    print('> nrmse:{:.4f}, std:{:.4f}'.format(np.mean(nrmse_), np.std(nrmse_)))\n",
    "\n",
    "    return {\n",
    "        'mae': (np.mean(mae_), np.std(mae_)),\n",
    "        'mse': (np.mean(mse_), np.std(mse_)),\n",
    "        'rmse': (np.mean(rmse_), np.std(rmse_)),\n",
    "        'mape': (np.mean(mape_), np.std(mape_)),\n",
    "        'rse': (np.mean(rse_), np.std(rse_)),\n",
    "        'corr': (np.mean(corr_), np.std(corr_)),\n",
    "        'nrmse': (np.mean(nrmse_), np.std(nrmse_))\n",
    "    }\n",
    "    \n",
    "results = run_experiments(iterations, random_seed, model, dataset, features, seq_len,\n",
    "                            label_len, pred_len, learning_rate, batch_size, inverse, tag, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
