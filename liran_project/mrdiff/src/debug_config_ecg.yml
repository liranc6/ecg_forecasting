wandb:
  mode: 'disabled'  # [online, disabled, offline]
  resume: 'None'  # ["allow", "must", "never", "auto", "None"]
  id: 'None'  # wandb run id

# optimization:
  batch_size: 16  # batch size of train input data
  test_batch_size: 32  # batch size of test input data

resume_exp:
  resume: False
  
data:
  norm_method: 'None'  # ['None', 'z_score', 'min_max']

training:

  # sequence:
    # len = minutes * seconds * fs
    # context_len: 0  # in minutes # input sequence length of SCINet encoder, look back window
    # seq_len: 56250 # input_length; window size
    # label_len: 52500  # I think its context len # in minutes # start token length of Informer decoder
    # pred_len: 3750  # I think its label/forecast len # prediction sequence length, horizon

  iterations:
    train_epochs: 40  # train epochs
    sample_times: 1

  patients:
    start_patient: 0
    end_patient: 25

  # sequence:
    # len = minutes * seconds * fs
    # context_len: 0  # in minutes # input sequence length of SCINet encoder, look back window
    # seq_len: 112500 # input_length; window size
    # label_len: 105000  # I think its context len # in minutes # start token length of Informer decoder
    # pred_len: 7500  # I think its label/forecast len # prediction sequence length, horizon

  # logging:
  #   sample: false
  #   log_interval: 2  # logging interval
  #   save_interval: 1  # save interval
  #   save_best: true  # save best model
  #   save_dir: '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff'  # save directory

validation:
  patients:
    start_patient: 0
    end_patient: 2

testing:
  patients:
    start_patient: 0
    end_patient: 2
