project_path: '/home/liranc6/ecg_forecasting'

tqdm: "terminal" # ["terminal", "notebook"]

resume_exp:
  resume: False  # resume training
  resume_from: false  # format: {run_id}?_step={step} for example: "o8p8z00y?_step=1000", when you resume a run from a specific step, it will start from that step, meaning it will redo the specified step.
  resume_optimizer: True
  resume_epoch: "None"
  loss_and_metrics: True
  resume_scheduler: True
  resume_configuration: True
  specific_chpt_path: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105_pl30_lr0.001_bs8_invFalse_itr0/09_10_2024_1410/best_checkpoint.pth"
  was_resumed: false

wandb:
  entity: 'liranc6'  # wandb entity
  mode: 'disabled'  # [online, disabled, offline]
  project: 'mrdiff'  # wandb project name
  resume: 'None'  # ["allow", "must", "never", "auto", "None"]
  run_name: "None" # wandb run name
  id: 'None'  # wandb run id
  save_code: True  # save code to wandb
  resume_from: "None"  # format: {run_id}?_step={step} for example: "o8p8z00y?_step=1000", when you resume a run from a specific step, it will start from that step, meaning it will redo the specified step.

general:
  random_seed: 42  # random seed
  evaluate: false  # true/false
  tag: null  # Optional
  dataset: 'icentia11k'  # ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2', 'electricity', 'solar_AL', 'exchange_rate', 'traffic', 'PEMS03', 'PEMS04', 'PEMS07', 'PEMS08']
  features: 'S'  # S is univariate, M is multivariate
  training_mode: 'ONE'
  interval: 1000  # number of diffusion steps

optimization:
  learning_rate: 0.001  # optimizer learning rate
  batch_size: 20  # batch size of train input data
  test_batch_size: 32  # batch size of test input data
  patience: 10  # early stopping patience
  weight_decay: 0.00001  # weight_decay
  lradj: '3'  # adjust learning rate
  pct_start: 0.3  # Percentage of training where learning rate increases

hardware:
  print_gpu_memory_usage: False  # print gpu memory usage
  num_workers: 0  # data loader num workers
  use_gpu: true  # use gpu
  gpu: 0  # gpu
  use_multi_gpu: false  # use multiple gpus
  devices: '0'  # device ids of multiple gpus

paths:
  train_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/train/p0_to_p32.h5'  # location of training data
  val_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/val/p33_to_p39.h5'  # location of validation data
  test_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/10minutes/test/p40_to_p46.h5'  # location of test data
  output_dir: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff"
  checkpoints: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff"
  model_path: "None" # "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105000_pl30000_lr0.001_bs8_invFalse_itr0/05_10_2024_1202/checkpoint.pth"

  
data:
  fs: 250
  freq: 'h'  # Options: [s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], or more detailed freq like 15min or 3h
  embed: 'timeF'  # Options: [timeF, fixed, learned]
  cols: []  # file list
  target: -1  # target feature
  inverse: false  # denorm the output data
  individual: true  # DLinear: a linear layer for each variate(channel) individually
  use_ar_init: false  # use autoregressive initialization
  use_residual: true
  uncertainty: false
  norm_method: 'None'  # ['None', 'z_score', 'min_max']
  normtype: 0


training:

  logging:
    sample: false
    log_interval: 5  # logging interval
    save_interval: 1  # save interval
    save_best: true  # save best model
    save_dir: '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff'  # save directory

  patients:
    start_patient: 0
    end_patient: 1

  iterations:
    itr: 1  # experiments times
    pretrain_epochs: 0  # train epochs
    train_epochs: 1  # train epochs
    sample_times: 1

  identifiers:
    id_worst: -1
    focus_variate: -1

  sequence:
    # len = minutes * seconds * fs
    context_len: 10  # in minutes # input sequence length of SCINet encoder, look back window
    seq_len: 135 # input_length; window size
    label_len: 120  # I think its context len # in minutes # start token length of Informer decoder
    pred_len: 15  # I think its label/forecast len # prediction sequence length, horizon

  model_info:
    opt_loss_type: 'mse'
    model: 'DDPM'  # model of the experiment
    base_model: 'Linear'  # model of the experiment  # deprecated? I didn't see it in the code
    u_net_type: 'v0'

  analysis:
    vis_MTS_analysis: 0  # status
    use_window_normalization: true
    use_future_mixup: true
    use_X0_in_THiDi: false  # Trend and Hierarchical Diffusion
    channel_independence: false

  smoothing:
    smoothed_factors: [5, 25, 51]  # List of smoothed factors, [5, 25, 51] are the kernel sizes for the smoothing

  ode:
    ot_ode: true  # use OT-ODE model
    beta_max: 1.0  # max diffusion for the diffusion model
    t0: 1e-4  # sigma start time in network parametrization
    T: 0.02  # sigma end time in network parametrization
    nfe: 20

  ablation_study:
    ablation_study_F_type: 'Linear'  # Linear, CNN
    beta_schedule: 'cosine'
    beta_dist_alpha: -1
    ablation_study_masking_type: 'none'  # none, hard, segment
    ablation_study_masking_tau: 0.9
    
  diffusion:
    beta_start: 0.0001
    beta_end: 0.02
    diff_steps: 100  # number of diffusion steps
    ddpm_inp_embed: 64  # ddpm_num_channels
    ddpm_layers_inp: 10
    ddpm_dim_diff_steps: 256
    ddpm_channels_conv: 128
    ddpm_channels_fusion_I: 256
    ddpm_layers_I: 5
    ddpm_layers_II: 10
    kernel_size: 25
    dec_channel_nums: 256
    cond_ddpm_num_layers: 5
    cond_ddpm_channels_conv: 256

  sampler:
    type_sampler: 'dpm'  # ["none", "dpm"]
    parameterization: 'x_start'  # ["noise", "x_start"]
    our_ddpm_clip: 100  # 100

  misc:
    affine: 0  # RevIN-affine; True 1 False 0
    subtract_last: 0  # 0: subtract mean; 1: subtract last
    subtract_short_terms: 0  # 0: subtract mean; 1: subtract last

validation:
  patients:
    start_patient: 0
    end_patient: 1

testing:
  patients:
    start_patient: 0
    end_patient: 1
