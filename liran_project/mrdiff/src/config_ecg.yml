project_path: '/home/liranc6/ecg_forecasting'

tqdm: "terminal" # ["terminal", "notebook"]
print_tqdm: False

debug: False  # debug mode
  # debug_config_path: "/home/liranc6/ecg_forecasting/liran_project/mrdiff/src/debug_config_ecg.yml"

pytorch_lightning:
  # Notes: when using DDP the code currently does not print how many GPUs are being used, it will only print "CUDA_VISIBLE_DEVICES: [0,1,...]"
        # Though, we can see it used multiple GPUs by reduction in number of batches pre epoch
  strategy: 'deepspeed_stage_2'  # [FSDP, DDP, DeepSpeed, deepspeed_stage_1, deepspeed_stage_2]

emd:
  use_emd: False
  num_sifts: 4
  max_iter: 1000
  tol: 1e-6

resume_exp:
  resume: False  # resume training
  resume_from: 9  # format: {run_id}?_step={step} for example: "o8p8z00y?_step=1000", when you resume a run from a specific step, it will start from that step, meaning it will redo the specified step.
  resume_optimizer: True
  resume_epoch: 9
  resume_metrics: True
  resume_scheduler: True
  resume_configuration: False
  resume_path: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl55000_ll3750_pl3750_lr0.0001_bs40_invTrue_itr0/21_11_2024_1807/epoch=10-vali_loss=0.02.ckpt"
  was_resumed: false
  model_start_training_time: "None"  # "%d_%m_%Y_%H%M" or "None" for example: "15_10_2024_1615"

wandb:
  entity: 'liranc6'  # wandb entity
  mode: 'online'  # [online, disabled, offline]
  project: 'mrdiff'  # wandb project name
  resume: 'None'  # ["allow", "must", "never", "auto", "None"]
  run_name: "None" # wandb run name
  id: 'None'  # wandb run id
  save_code: False  # save code to wandb
  resume_from: "None"  # format: {run_id}?_step={step} for example: "o8p8z00y?_step=1000", when you resume a run from a specific step, it will start from that step, meaning it will redo the specified step.

general:
  random_seed: 42  # random seed
  evaluate: false  # true/false
  tag: null  # Optional
  dataset: 'icentia11k'  # ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2', 'electricity', 'solar_AL', 'exchange_rate', 'traffic', 'PEMS03', 'PEMS04', 'PEMS07', 'PEMS08', 'icentia11k']
  features: 'S'  # S is univariate, M is multivariate
  training_mode: 'ONE'
  interval: 1000  # number of diffusion steps?

optimization:
  learning_rate: 0.001  # optimizer learning rate
  batch_size: 64  # batch size of train input data
  test_batch_size: 64  # batch size of test input data
  patience: 35  # early stopping patience
  weight_decay: 0.00001  # weight_decay
  lradj: '3'  # adjust learning rate (Learning Rate ADJustment)  # ['TST', 'type1', 'type2', 'type3', 'constant', '1', '2', '3', '4', '5']
  pct_start: 0.3  # Percentage of training where learning rate increases
  accum_iter: 4

hardware:   
  print_gpu_memory_usage: False  # print gpu memory usage
  num_workers: 0  # data loader num workers
  use_gpu: true  # use gpu
  gpu: 0  # gpu
  use_multi_gpu: True  # use multiple gpus
  device_ids: '0,1'  # device ids of multiple gpus
  memory_snapshot: True

paths:
  train_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/15_minutes/train/p0_to_p399.h5'  # location of training data
  val_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/15_minutes/val/p400_to_p449.h5'  # location of validation data
  test_data: '/home/liranc6/data/with_R_beats/icentia11k-continuous-ecg_normal_sinus_subset_npArrays_splits/15_minutes/test/p450_to_p500.h5'  # location of test data
  output_dir: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff"
  checkpoints: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff"
  model_path: "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl55000_ll3750_pl3750_lr0.0001_bs40_invTrue_itr0/21_11_2024_1807/epoch=10-vali_loss=0.02.ckpt" # "/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff/DDPM_icentia11k_ftS_sl10_ll105000_pl30000_lr0.001_bs8_invFalse_itr0/05_10_2024_1202/checkpoint.pth"
  debug_config_path: "/home/liranc6/ecg_forecasting/liran_project/mrdiff/src/debug_config_ecg.yml"
  norm_statistics_file: "/home/liranc6/ecg_forecasting/liran_project/utils/data_statistics/icentia11k/0to399.pkl"

data:
  fs: 250
  # freq: 'h'  # Options: [s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], or more detailed freq like 15min or 3h
  embed: 'timeF'  # Options: [timeF, fixed, learned]
  cols: []  # file list
  target: -1  # target feature
  inverse: True  # denorm the output data
  individual: False  # DLinear: a linear layer for each variate(channel) individually
  use_ar_init: false  # use autoregressive initialization
  use_residual: true
  uncertainty: false
  norm_method: 'z_score'  # ['None', 'z_score', 'min_max']
  normtype: 0


training:

  data:
    step: 4  # subsample rate

  logging:
    sample: false
    log_interval: 2  # logging interval
    save_interval: 1  # save interval
    save_best: true  # save best model
    save_dir: '/home/liranc6/ecg_forecasting/liran_project/results/icentia11k/mrDiff'  # save directory
    log_start_epoch: 10

  patients:
    start_patient: 0
    end_patient: 280

  iterations:
    itr: 1  # experiments times
    pretrain_epochs: 0  # pretrain epochs
    train_epochs: 300  # train epochs
    sample_times: 1

  identifiers:
    id_worst: -1
    focus_variate: -1

  sequence:
    # len = minutes * seconds * fs
    context_len: 0  # in minutes # input sequence length of SCINet encoder, look back window
    seq_len: 63000 # input_length; window size
    label_len: 60000  # I think its context len # in minutes # start token length of Informer decoder
    pred_len: 3000  # I think its label/forecast len # prediction sequence length, horizon

  model_info:
    add_loss: ['dtw'] # ['dtw', 'ctc']
    opt_loss_type: 'mse'
    model: 'DDPM'  # model of the experiment
    base_model: 'Lin√∏ear'  # model of the experiment  # deprecated? I didn't see it in the code
    u_net_type: 'v0'

  analysis:
    vis_MTS_analysis: 0  # status
    use_window_normalization: true
    use_future_mixup: true
    use_X0_in_THiDi: false  # Trend and Hierarchical Diffusion
    channel_independence: false

  smoothing:
    reverse_order: False  # reverse order of the smoothed factors
    smoothed_factors: [] #[11, 19, 27, 35]  # mast be odd (non-even) numbers. List of smoothed factors, [5, 25, 51] are the kernel sizes for the smoothing

  ode:
    ot_ode: true  # use OT-ODE model
    beta_max: 1.0  # max diffusion for the diffusion model
    t0: 1e-4  # sigma start time in network parametrization
    T: 0.02  # sigma end time in network parametrization
    nfe: 20  # number of function evaluations

  ablation_study:
    ablation_study_F_type: 'Linear'  # Linear, CNN
    beta_schedule: 'cosine'
    beta_dist_alpha: -1
    ablation_study_masking_type: 'none'  # none, hard, segment
    ablation_study_masking_tau: 0.9
    
  diffusion:
    beta_start: 0.0001  # Initial value of the beta parameter in the diffusion process, controlling the noise level at the start.
    beta_end: 0.02  # Final value of the beta parameter in the diffusion process, controlling the noise level at the end.
    diff_steps: 100  # Number of diffusion steps, determining how many steps the diffusion process will take.
    ddpm_inp_embed: 64  # Embedding dimension for the input in the DDPM, also referred to as ddpm_num_channels.
    ddpm_dim_diff_steps: 128  # Dimensionality of the diffusion steps in the DDPM.
    ddpm_channels_conv: 128  # Number of convolutional channels used in the DDPM.
    ddpm_channels_fusion_I: 128  # Number of channels in the first fusion layer of the DDPM.
    cond_ddpm_num_layers: 5  # Number of layers in the conditional DDPM.
    ddpm_layers_inp: 7  # Number of layers in the input embedding part of the DDPM.
    ddpm_layers_I: 5  # Number of layers in the first part of the DDPM.
    ddpm_layers_II: 5  # Number of layers in the second part of the DDPM.
    kernel_size: 25  # Size of the convolutional kernels used in the DDPM.
    stride: 1  # Stride of the convolutional kernels used in the DDPM.
    use_default_init_k_ans_s: true  # Use default initialization for the kernel and stride in the DDPM.
    dec_channel_nums: 256  # Number of channels in the decoder part of the DDPM.
    cond_ddpm_channels_conv: 256  # Number of convolutional channels in the conditional DDPM.

  sampler:
    type_sampler: 'dpm'  # ["none", "dpm"]
    parameterization: 'x_start'  # ["noise", "x_start"]
    our_ddpm_clip: 100  # 100

  misc:
    affine: 0  # RevIN-affine; True 1 False 0
    subtract_last: 0  # 0: subtract mean; 1: subtract last
    subtract_short_terms: 0  # 0: subtract mean; 1: subtract last

validation:
  patients:
    start_patient: 0
    end_patient: 28
  data:
    step: 4  # subsample rate

testing:
  patients:
    start_patient: 0
    end_patient: 7
  data:
    step: 4  # subsample rate
